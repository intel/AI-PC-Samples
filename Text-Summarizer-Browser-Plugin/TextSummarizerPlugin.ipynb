{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a9a997-e2e8-43cf-9e40-5c777b4b9723",
   "metadata": {},
   "source": [
    "# Text Summarizer Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99707adf-f1be-4e00-aad4-8187f8c816cf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Chrome extension seamlessly integrates with Flask and leverages an OpenVINO backend for fast and efficient summarization of webpages (via URL) and PDFs (via upload). Powered by LangChain tools, it handles advanced tasks like text splitting and vectorstore management to deliver accurate and meaningful summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b6e8b-fcda-4fd1-9f4f-baf028233ea0",
   "metadata": {},
   "source": [
    "## How it Works\n",
    "\n",
    "<img width=\"1000\" alt=\"image\" src=\"./assets/Text-Summarizer-Overview.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ee514-4487-45d3-984e-6f6e291b61f0",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07f0ea0-c374-4048-b907-bf8a9fd43a9b",
   "metadata": {},
   "source": [
    "### Follow the below steps to prepare the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ef7d9-b2d0-4b38-9829-f92224ee9998",
   "metadata": {},
   "source": [
    "Before converting the models & running the plugin, make sure you have followed all the below listed [steps to prepare the environment](./Readme.md/#setup-environment-with-uv)\n",
    "- Install uv by refering to [this documentation](https://docs.astral.sh/uv/getting-started/installation/)\n",
    "- Sync dependencies\n",
    "  ```bash\n",
    "  uv sync\n",
    "  ```\n",
    "Load and pin the unpacked extension as mentioned [here](./Readme.md/):\n",
    "- Navigate to the Extensions page by entering chrome://extensions in a new tab. (By design chrome:// URLs are not linkable.)\n",
    "  - Alternatively, click the Extensions menu puzzle button and select Manage Extensions at the bottom of the menu.\n",
    "  - Or, click the Chrome menu, hover over More Tools, then select Extensions.\n",
    "- Enable Developer Mode by clicking the toggle switch next to Developer mode.\n",
    "- Click the Load unpacked button and select the extension directory.\n",
    "- Refer to Chromeâ€™s development documentation for further details.\n",
    "- Pin your extension to the toolbar for a quick access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc939a97-428b-4c61-93ba-51cfce27c54f",
   "metadata": {},
   "source": [
    "### Download and Convert the Huggingface Model to OpenVINO IR Format:\n",
    "\n",
    "#### Login to Huggingface:\n",
    "- Generate a token from Huggingface for private/gated models like Meta Llama, etc. To access such private/gated models, refer to [Huggingface documentation](https://huggingface.co/docs/hub/en/models-gated).\n",
    "- If you already have access to such gated models, know how to generate an access token by referring [this](https://huggingface.co/docs/hub/en/security-tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c9347f7-09dc-4324-b1e2-253c4d5a9f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf794ab80beb4fa3a75f1678de91ad40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebaae15-d10f-47ed-b2f6-8d2a290e92ff",
   "metadata": {},
   "source": [
    "#### Converting a huggingface model to OpenVINO\n",
    "Convert the models using `optimum-cli`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d30ece-09c1-4d6c-8566-b252d7b6a4c7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> Converting LLMs to OpenVINO IR Format could take sometime.. Meanwhile, have a milkshake!ðŸ§‹ </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a146b7-2aad-43be-8a8a-68bb1d885a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "ROOT_DIR = Path.cwd()\n",
    "MODEL_DIR = ROOT_DIR / 'models'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.system(f\"uv run optimum-cli export openvino --model Qwen/Qwen2-7B-Instruct --weight-format int4 {MODEL_DIR}/ov_qwen7b\")\n",
    "os.system(f\"uv run optimum-cli export openvino --model meta-llama/Llama-2-7b-chat-hf --weight-format int4 {MODEL_DIR}/ov_llama_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367fa4b-d96a-4447-a26f-8a50544e908a",
   "metadata": {},
   "source": [
    ">**Note**: [Raise access request](https://www.llama.com/llama-downloads) for Llama models as it is a gated repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d060a9e-0355-41e5-a33c-c8baf35d7481",
   "metadata": {},
   "source": [
    "## Backend code for Text Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de840a1b-5913-455c-bb2e-fe90f4ebb545",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1333e1-1785-4a4b-b426-a7d32640236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "import os\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43d5d3-88bf-4bd6-94bd-17c8ffb9006f",
   "metadata": {},
   "source": [
    "### Prompt Templates for Summarization & Question Answering Bot\n",
    "Here we have declared two variables for prompt templates so that it can be called later on , one template for summarization and one for query asked in the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8469f9-5b72-4a2c-bc09-a4406a34a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt template for summarization\n",
    "summary_template= \"\"\"Write a concise summary of the following: \"{context}\" CONCISE SUMMARY: \"\"\"\n",
    "#prompt template for query\n",
    "query_template=\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use 10 words maximum and keep the answer as concise as possible in one sentence.\n",
    "    Always say \"thanks for asking!\" at the end of the answer.\n",
    " \n",
    "    {context}\n",
    " \n",
    "    Question: {question}\n",
    " \n",
    "    Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8958c7f-7241-4784-a6a9-0d59431b0e4e",
   "metadata": {},
   "source": [
    "### Pre-process the Input File\n",
    "\n",
    "* Through browser plugin, users post an input file for summarization. \n",
    "* Document loaders, i.e. WebBaseLoader & PyPDFLoader, in RAG are used to load page content from any Webpage/PDF and preprocess the documents that will be further used for retrieval during the summarization & question answering process.\n",
    "* The loaded page data would be split using Recursive Character Text Splitter & embeddings are created using HuggingFace Embeddings. Here, RecursiveCharacterTextSplitter is used to split text into smaller pieces recursively at the character level.\n",
    "* In RAG, embeddings plays a crucial role in retrieval of relevant documents for a given query and Sentence Transformers helps to generate embeddings for each document in your knowledge base.\n",
    "* These embeddings are further stored into ChromaDB for further retrieval usage. Chroma is a vector store and embeddings database designed from the ground-up to make it easy to build AI applications with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd83e322-d97f-420d-88cf-21ceba6261fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(loader):\n",
    "    \"\"\"\n",
    "        This function does the below steps in a sequential order:\n",
    "        1. Loads page content from the webpage/PDF \n",
    "        2. Splits the page data using Recursive Character Text Splitter & creates embeddings using HuggingFace Embeddings\n",
    "        3. This is further stored into ChromaDB for futher retrieval usage\n",
    "        input: loader contains page data from a Webpage/PDF\n",
    "        output: returns a vectorstore\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page_data = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "        all_splits = text_splitter.split_documents(page_data)\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        global vectorstore\n",
    "        vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)  \n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(\"Error while processing Webpage/PDF page content\\n\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0f347-df38-4c93-a77c-ab978c00cfaf",
   "metadata": {},
   "source": [
    "### Load LLM models\n",
    "Below module:\n",
    "1. Fetches the OpenVINO converted model & compiles it on GPU\n",
    "2. Generate a HuggingFace Pipeline for Text-Generation.\n",
    "3. Returns the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6750172c-0d22-4e70-98d5-7a5ec29f4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_id):\n",
    "    \"\"\"\n",
    "        Meta Llama2 & Qwen 7B models are converted to OpenVINO IR Format. This function compiles those converted models on GPU.\n",
    "        input: user selected model_id from plugin\n",
    "        output: compiled model with openvino\n",
    "    \"\"\"\n",
    "    if model_id:\n",
    "        try:\n",
    "            if model_id==\"Meta LLama 2\":\n",
    "                model_path='models/ov_llama_2'\n",
    "            elif model_id==\"Qwen 7B Instruct\":\n",
    "                model_path='models/ov_qwen7b'\n",
    "            model = OVModelForCausalLM.from_pretrained(model_path , device='GPU')\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            pipe=pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=4096,  \n",
    "                device=model.device\n",
    "            )\n",
    "            global llm_model \n",
    "            llm_model = HuggingFacePipeline(pipeline=pipe)\n",
    "            return llm_model\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load the model. Please check whether the model_path is correct\\n\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f8da99-d624-4ca9-aa38-f7fb6f7c3200",
   "metadata": {},
   "source": [
    "###  URL Summarization\n",
    "For a URL Summarization, we load the web page content when end-user enters a URL into the plugin using **WebBaseLoader** which in return loads the page data and passes into the RetrievalQA chain. When a question is being asked in the retreival QA chain , we try to get a concise summary and return it . Here we are using WebBaseLoader to load the documents from the web.\n",
    "* The **WebBaseLoader** in Retrieval Augmented Generation (RAG) is a type of document loader that is designed to load documents from the web.The WebBaseLoader is used when the documents for retrieval are not stored locally or in a Hugging Face dataset, but are instead located on the web.\n",
    "* **RetrievalQA** is a type of question answering system that uses a retriever to fetch relevant documents given a question, and then uses a reader to extract the answer from the retrieved documents.\n",
    "\n",
    "Below module:\n",
    "1. Loads the page data from a webpage using WebBaseLoader.\n",
    "2. Pre-processed the data & stores into a vector store.\n",
    "3. Passes the prompt, vectorstore & LLM model into the chain & returns the summary to the plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edebde7e-f6df-4123-88e8-13f9f072d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_url_data(urls):\n",
    "    \"\"\"\n",
    "        When an end user pastes a URL into the plugin, The RAW data is passed onto the RetrievalQA chain,\n",
    "        and the output is returned back to the plugin.\n",
    "        input: Webpage URL(str).\n",
    "        output: Glance Summary of the fetched URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = WebBaseLoader(urls)\n",
    "        global summ_vectorstore \n",
    "        summ_vectorstore = pre_processing(loader) # Common Helper function for processing data.\n",
    "        prompt = PromptTemplate(\n",
    "            template=summary_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "    \n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm_model,\n",
    "            retriever=summ_vectorstore.as_retriever(),\n",
    "            chain_type=\"stuff\",\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=False,\n",
    "        )\n",
    "        \n",
    "        question = \"Please summarize the entire book in one paragraph of 100 words\"\n",
    "        summary = qa_chain(question)\n",
    "        response = summary['result']\n",
    "        summary_start = response.find(\"CONCISE SUMMARY:\")\n",
    "        concise_summary = response[summary_start + len(\"CONCISE SUMMARY:\"):].strip()\n",
    "        return concise_summary\n",
    "    except Exception as e:\n",
    "        print(\"Failed to summarize webpage\\n\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe1bad-94b8-4afd-a906-235a532affb3",
   "metadata": {},
   "source": [
    "### URL Question Answering BOT\n",
    "The below module:\n",
    "1. Fetches any follow up questions related to the summary post URL summarization.\n",
    "2. Passes the **query template** which is declared as global into the prompt, vectorstore, compiled model to the chain.\n",
    "3. When a question is being passed to the retrieval QA chain, the chain loads & posts a precise answer in a single sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6ad4d92-c524-450b-b8af-e5630ed0b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_on_url_summarized_text(query):\n",
    "    \"\"\"\n",
    "        This function fetches the query asked by the users post summarization from the URL, searches an answer from the vectorstore & returns answer in less than 10 words.\n",
    "        input: user's follow-up question(str)\n",
    "        output: Answer to the conversations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = PromptTemplate(\n",
    "            template=query_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "        reduce_chain = RetrievalQA.from_chain_type(\n",
    "                llm=llm_model,\n",
    "                retriever=summ_vectorstore.as_retriever(),\n",
    "                chain_type=\"stuff\",\n",
    "                chain_type_kwargs={\"prompt\": prompt},\n",
    "                return_source_documents=False\n",
    "            )\n",
    "        summary = reduce_chain({'query': query})\n",
    "        summ_vectorstore.delete\n",
    "        response = summary['result']\n",
    "        summary_start = response.find(\"Helpful Answer:\")\n",
    "        concise_summary = response[summary_start + len(\"Helpful Answer:\"):].strip()\n",
    "        return concise_summary\n",
    "    except Exception as e:\n",
    "        print(\"Error in Webpage Summarizer QA BoT\\n\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401468ef-6d4c-48e2-832f-3e8886aa38c8",
   "metadata": {},
   "source": [
    "### PDF Summarization\n",
    "\n",
    "When end-users upload any PDF file to the plugin, page data is loaded, using **PyPDFLoader** and passed into the RetrievalQA chain for generating a concise summary. When a question is being asked post summarization, a precise answer is returned. \n",
    "\n",
    "* **PyPDFLoader** is a document loader within the LangChain framework specifically designed to handle PDF files. It allows you to extract text from PDF documents and load them into a format suitable for language models and other text-based applications.\n",
    "\n",
    "Below module: \n",
    "1. Loads the page data from a webpage using PyPDFLoader.\n",
    "2. Pre-processed the data & stores into a vector store.\n",
    "3. Passes the prompt, vectorstore & LLM model into the chain & returns the summary to the plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83fc83ad-3e4a-4d4b-906e-01bbebd10ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_pdf_data(pdf):\n",
    "    \"\"\"\n",
    "        When an end user uploads a PDF into the plugin, The RAW data is passed onto the RetrievalQA chain,\n",
    "        and the output is returned back to the plugin.\n",
    "        input: PDF path(str).\n",
    "        output: Glance Summary of the uploaded PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf, extract_images=False)\n",
    "        global pdf_vectorstore\n",
    "        pdf_vectorstore=pre_processing(loader)\n",
    "    \n",
    "        prompt = PromptTemplate(\n",
    "            template=summary_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        reduce_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm_model,\n",
    "            retriever=pdf_vectorstore.as_retriever(),\n",
    "            chain_type=\"stuff\",\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=False,\n",
    "        )\n",
    "        question = \"Please summarize the entire book in 100 words.\"\n",
    "        summary = reduce_chain({'query': question})\n",
    "\n",
    "        response = summary['result']\n",
    "        summary_start = response.find(\"CONCISE SUMMARY:\")\n",
    "        concise_summary = response[summary_start + len(\"CONCISE SUMMARY:\"):].strip()\n",
    "        return concise_summary\n",
    "    except Exception as e:\n",
    "        print(\"Failed to summarize PDF \\n\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c21bf-4e16-432b-a7e1-7ca72b690020",
   "metadata": {},
   "source": [
    "### PDF Question Answering BOT\n",
    "The below module:\n",
    "1. Posts follow up questions, asked by end-users, related to the summary PDF post PDF summarization\n",
    "2. Passes the **query template** which is declared as global into the prompt, vectorstore, compiled model to the chain.\n",
    "3. When a question is being passed to the retrieval QA chain, the chain loads & posts a precise answer in a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd5d11d-0e12-4e1e-9669-31692af50159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_on_pdf_summarized_text(query):\n",
    "    \"\"\"\n",
    "        This function fetches the query asked by the users post summarization from the PDF, then after it searches an answer from the vectorstore & returns answer in less than 10 words.\n",
    "        input: user's follow-up question(str)\n",
    "        output: Answer to the conversations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = PromptTemplate(\n",
    "            template=query_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "        reduce_chain = RetrievalQA.from_chain_type(\n",
    "                llm=llm_model,\n",
    "                retriever=pdf_vectorstore.as_retriever(),\n",
    "                chain_type=\"stuff\",\n",
    "                chain_type_kwargs={\"prompt\": prompt},\n",
    "                return_source_documents=False\n",
    "            )\n",
    "        summary = reduce_chain({'query': query})\n",
    "        response = summary['result']\n",
    "        summary_start = response.find(\"Helpful Answer:\")\n",
    "        concise_summary = response[summary_start + len(\"Helpful Answer:\"):].strip()\n",
    "        return concise_summary\n",
    "    except Exception as e:\n",
    "        print(\"Error in PDF Summarizer QA BoT\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325c0ec-aeb7-492c-8396-0cc5a4c21cb9",
   "metadata": {},
   "source": [
    "## Server-side code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060492e-e311-4bf1-ad3c-1c31c55bda93",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1434779c-1d0b-49ec-bbb9-dbbd827f61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from flask import Flask, Response, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import tempfile\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb527c-f34f-4d1d-92f9-6d7bdad50dee",
   "metadata": {},
   "source": [
    "### Initializing the flask app and enabling CORS\n",
    "Here we are initializing a flask and enabling CORS which allows the flask app tobe accessed and interacted with from other domains and we are restricting the types of files that can be uploaded to the application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54773857-d296-44c4-847e-d26da86b01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)  # This will enable CORS for all routes\n",
    "ALLOWED_EXTENSIONS = {'txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72ba5f-b056-4601-a0fc-d963c0cc36ad",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "The below module:\n",
    "1. Fetches the model selected by the end-user through model_id.\n",
    "2. Loads the model which would further trigger the model compilation function present in the main code for summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354d0e26-8719-4b54-888d-0264c0954416",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/select-model', methods=['POST'])\n",
    "def select_model():\n",
    "    \"\"\"\n",
    "        Model selection function which would further trigger Model compilation function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        global current_model\n",
    "        data = request.get_json()\n",
    "        model_id = data.get('model_id')\n",
    "        current_model = load_llm(model_id)\n",
    "        return jsonify({'message': f'Model {model_id} loaded successfully.'}), 200\n",
    "    \n",
    "    except Exception:\n",
    "        return jsonify({'message': f'Failed to load model'}), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69e9b5-ddf7-470f-ad93-cffea37f793a",
   "metadata": {},
   "source": [
    "### Yielding Summary onto the plugin\n",
    "This is a generator function which helps to Stream and yield the response content chunk by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f664170-bf7c-41b5-b41b-45217a5c1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_output(process_function, *args):\n",
    "    \"\"\"\n",
    "        Generator function to stream output from a process function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for chunk in process_function(*args):\n",
    "            if chunk is not None:\n",
    "                yield f\"{chunk}\"\n",
    "    except Exception:\n",
    "        yield f\"Error while streaming output\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789173e-f5f3-4c47-b576-d167a70a21b0",
   "metadata": {},
   "source": [
    "### URL processing code\n",
    "This will fetch the URL from the user's input from the plugin and trigger the URL summarization function present in the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36575ff6-f3ee-4ddc-8c56-689c8b6844bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/process-url', methods=['POST'])\n",
    "def process_url():\n",
    "    \"\"\"\n",
    "        Fetches URL from the plugin & triggers the URL summarization function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        url = data.get('url')  \n",
    "        if not url:\n",
    "            return jsonify({'message': 'No URL provided'}), 400\n",
    "        chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "        return Response(stream_output(pre_process_url_data, [url]), content_type='text/event-stream')\n",
    "    \n",
    "    except Exception: \n",
    "        return jsonify({'message': f'Error while processing URL'}), 400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833feed5-8e31-4c1f-8bb2-598322208755",
   "metadata": {},
   "source": [
    "### PDF processing code\n",
    "This function takes the PDF uploaded by the user and trigger the PDF summarization function present in the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea9ce78-c713-4204-ad68-9b9a9e9a79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/upload-pdf', methods=['POST'])\n",
    "def upload_pdf():\n",
    "    \"\"\"\n",
    "        Once the PDF's uploaded, the PDF Summarization function's triggered.\n",
    "    \"\"\"\n",
    "   \n",
    "    pdf_file = request.files['pdf'] \n",
    "    if pdf_file and pdf_file.content_type == 'application/pdf':\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_pdf:\n",
    "                pdf_file.save(temp_pdf.name)\n",
    "                temp_pdf_path = temp_pdf.name\n",
    "                print(temp_pdf_path)\n",
    "           \n",
    "            chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "            return Response(stream_output(pre_process_pdf_data, temp_pdf_path), content_type='text/event-stream')\n",
    " \n",
    "        except Exception:\n",
    "            return jsonify({\"message\": f\"Error processing PDF:\"}), 500\n",
    " \n",
    "    else:\n",
    "        return jsonify({\"message\": \"Invalid file type. Please upload a PDF.\"}), 400\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56256d03-048c-4dfe-ae5f-2e7b53f0aa66",
   "metadata": {},
   "source": [
    "### PDF Query code for Question Answering Bot\n",
    "Once the PDF content summarization is done user asks query to the Question Answering bot which gets triggered to the Query for the PDF function present in the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b4f6d9e-9007-461c-88ed-13260d5cf559",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/your_query_pdf', methods=['POST'])\n",
    "def pdf_process_query():\n",
    "    \"\"\"\n",
    "        This function triggers the PDF Question Answering Bot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        query=data.get('query')\n",
    "        if not data:\n",
    "            return jsonify({'message':'no query provided'}),400\n",
    "        response_message=str(qa_on_pdf_summarized_text(query))\n",
    "        return jsonify({'message': response_message}), 200\n",
    "    except Exception:\n",
    "        return jsonify({'message': f'Error while PDF QA Bot'}), 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d6199-fafa-4340-9c08-af63b23f15a2",
   "metadata": {},
   "source": [
    "### URL Query code for Question Answering Bot\n",
    "Once the URL content summarization is done user asks query to the Question Answering bot which gets triggered to the Query for the URL function present in the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "873be699-2bf8-4238-88cd-f3c36431fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/your_query_url', methods=['POST'])\n",
    "def url_process_query():\n",
    "    \"\"\"\n",
    "        This function triggers the URL question answering Bot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        query=data.get('query')\n",
    "        if not data:\n",
    "            return jsonify({'message':'no query provided'}),400\n",
    "        response_message=str(qa_on_url_summarized_text(query))\n",
    "        return jsonify({'message': response_message}), 200\n",
    "    except Exception:\n",
    "        return jsonify({'message': f'Error while URL QA Bot'}), 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1be65-2d3b-4fce-bb31-baa7a6573e25",
   "metadata": {},
   "source": [
    "### Launch Plugin via Flask Server\n",
    "\n",
    "#### ðŸŽ‰ Flask server is Ready! ðŸŽ‰\n",
    "\n",
    "Your application is now live and waiting for interaction!\n",
    "\n",
    "**ðŸš€ Essential Step: Activate Your Browser Plugin!**\n",
    "\n",
    "- This application operates through its dedicated browser extension.\n",
    "- To begin, please open your web browser and locate the plugin's icon, which looks like `T`, in your toolbar (it's often in the top-right corner).\n",
    "- Click on the `T` icon to access the browser extension\n",
    "  > **NOTE**: Do not access the flask server API Endpoint as the application is running on **browser extension**. \n",
    "\n",
    "**Having trouble?**\n",
    "- Is the plugin loaded? If you haven't already, please load it by following the Readme.md file.\n",
    "- Is it enabled? Double-check your browser's extension settings to ensure the plugin isn't disabled.\n",
    "- Have you pinned the extension? Pin the extension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4138fa1d-5f11-4629-a636-f17cc114644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "Device set to use cpu\n",
      "127.0.0.1 - - [03/Jun/2025 03:55:06] \"POST /select-model HTTP/1.1\" 200 -\n",
      "Device set to use cpu\n",
      "127.0.0.1 - - [03/Jun/2025 03:55:39] \"POST /select-model HTTP/1.1\" 200 -\n",
      "Device set to use cpu\n",
      "127.0.0.1 - - [03/Jun/2025 03:56:13] \"POST /select-model HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Jun/2025 03:56:40] \"POST /process-url HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5379a7-3e4e-4dbb-acbc-145f731318e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0e9d12adff82465199bd70fa07b43072": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "12c4105bc2a945b19d326da722325528": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3f3b46837d5b4395af1756d697e6952e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3f5b562a7ad34f289cef403e03953cb0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "62913bd218dc4b2db72839e840b3e951": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b88a5750696b4d298d1af024a7910fd3",
       "style": "IPY_MODEL_d039a936168b48ae88bc2874f2b15af4",
       "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
      }
     },
     "634a99b16e654ba198b858f8646c5351": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "PasswordModel",
      "state": {
       "description": "Token:",
       "layout": "IPY_MODEL_12c4105bc2a945b19d326da722325528",
       "style": "IPY_MODEL_e6add341f25942cab072d70bead621d0"
      }
     },
     "664c910d38d848759b804d5750ea5a3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6aa4f1fa34914b7b916a4173662b7675": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "align_items": "center",
       "display": "flex",
       "flex_flow": "column",
       "width": "50%"
      }
     },
     "9d4f6f328115401ab5db50e324e1eee8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Login",
       "layout": "IPY_MODEL_3f5b562a7ad34f289cef403e03953cb0",
       "style": "IPY_MODEL_a3665d6c59d04668938f48e41fa8ed09",
       "tooltip": null
      }
     },
     "a077624af4f74afa83fe376631054922": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d854e0c618a54c3b9f16482111c77e58",
       "style": "IPY_MODEL_0e9d12adff82465199bd70fa07b43072",
       "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
      }
     },
     "a3665d6c59d04668938f48e41fa8ed09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "b88a5750696b4d298d1af024a7910fd3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cf794ab80beb4fa3a75f1678de91ad40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_62913bd218dc4b2db72839e840b3e951",
        "IPY_MODEL_634a99b16e654ba198b858f8646c5351",
        "IPY_MODEL_d2c52d76a75c4fbb9172d1dc0692b0d1",
        "IPY_MODEL_9d4f6f328115401ab5db50e324e1eee8",
        "IPY_MODEL_a077624af4f74afa83fe376631054922"
       ],
       "layout": "IPY_MODEL_6aa4f1fa34914b7b916a4173662b7675"
      }
     },
     "d039a936168b48ae88bc2874f2b15af4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2c52d76a75c4fbb9172d1dc0692b0d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxModel",
      "state": {
       "description": "Add token as git credential?",
       "disabled": false,
       "layout": "IPY_MODEL_664c910d38d848759b804d5750ea5a3a",
       "style": "IPY_MODEL_3f3b46837d5b4395af1756d697e6952e",
       "value": true
      }
     },
     "d854e0c618a54c3b9f16482111c77e58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6add341f25942cab072d70bead621d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
