{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a9a997-e2e8-43cf-9e40-5c777b4b9723",
   "metadata": {},
   "source": [
    "# Text Summarizer Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99707adf-f1be-4e00-aad4-8187f8c816cf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Chrome extension seamlessly integrates with Flask and leverages an OpenVINO backend for fast and efficient summarization of webpages (via URL) and PDFs (via upload). Powered by LangChain tools, it handles advanced tasks like text splitting and vectorstore management to deliver accurate and meaningful summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b6e8b-fcda-4fd1-9f4f-baf028233ea0",
   "metadata": {},
   "source": [
    "## How it Works\n",
    "\n",
    "<img width=\"1000\" alt=\"image\" src=\"./assets/Text-Summarizer-Overview.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ee514-4487-45d3-984e-6f6e291b61f0",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13e251-d11d-4709-8068-29bdf38ed67d",
   "metadata": {},
   "source": [
    "### Install the below necessary tools/packages:\n",
    "   - [Git on Windows](https://git-scm.com/downloads)\n",
    "   - [Miniforge](https://conda-forge.org/download/)\n",
    "   - [Google Chrome for Windows](https://www.google.com/chrome/?brand=OZZY&ds_kid=43700080794581137&gad_source=1&gclid=Cj0KCQiAoae5BhCNARIsADVLzZdwNNB5nIyjZ8OyCzg6h_cCig1eoaYquUSEd7BAigJhTzps1Kxuop8aArE6EALw_wcB&gclsrc=aw.ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07f0ea0-c374-4048-b907-bf8a9fd43a9b",
   "metadata": {},
   "source": [
    "### Follow the below steps to prepare the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ef7d9-b2d0-4b38-9829-f92224ee9998",
   "metadata": {},
   "source": [
    "Before converting the models & running the plugin, make sure you have followed all the below listed [steps to prepare the environment](./README.md/#prerequisites)\n",
    "- Cloning the Text-Summarizer Plugin Repository\n",
    "- Creating conda environment & Installing necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc939a97-428b-4c61-93ba-51cfce27c54f",
   "metadata": {},
   "source": [
    "### Download and Convert the Huggingface Model to OpenVINO IR Format:\n",
    "\n",
    "#### Login to Huggingface:\n",
    "Generate a token from Huggingface for private/gated models like Meta Llama, etc. To access such private/gated models, refer to [Huggingface documentation](https://huggingface.co/docs/hub/en/models-gated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9347f7-09dc-4324-b1e2-253c4d5a9f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebaae15-d10f-47ed-b2f6-8d2a290e92ff",
   "metadata": {},
   "source": [
    "#### Converting a huggingface model to OpenVINO\n",
    "Convert the models using `optimum-cli`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a2914-f5ab-4464-9e1f-b3ee852eac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models && cd models     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c15312-3c8b-42d0-bc1c-ca4987124708",
   "metadata": {},
   "outputs": [],
   "source": [
    "! optimum-cli export openvino --model Qwen/Qwen2-7B-Instruct --weight-format int4 ov_qwen7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb72875-157c-4fab-9ecf-4ba1e15f5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "! optimum-cli export openvino --model meta-llama/Llama-2-7b-chat-hf --weight-format int4 ov_llama_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367fa4b-d96a-4447-a26f-8a50544e908a",
   "metadata": {},
   "source": [
    ">**Note**: [Raise access request](https://www.llama.com/llama-downloads) for Llama models as it is a gated repository.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b2ed7c2-e406-4423-b7a6-0a377bb2a4b5",
   "metadata": {},
   "source": [
    "### Load the extension\n",
    "\n",
    "To load an unpacked extension in developer mode:\n",
    "- Go to the Extensions page by entering **chrome://extensions** in a new tab. (By design chrome:// URLs are not linkable.)\n",
    "    - Alternatively, **click the Extensions menu puzzle button and select Manage Extensions** at the bottom of the menu.\n",
    "    - Or, click the Chrome menu, hover over More Tools, then select Extensions.\n",
    "- Enable **Developer Mode** by clicking the toggle switch next to Developer mode.\n",
    "- Click the **Load unpacked** button and select the extension directory.\n",
    "- Refer to [Chromeâ€™s development documentation](https://developer.chrome.com/docs/extensions/get-started/tutorial/hello-world#load-unpacked) for further details.\n",
    "\n",
    "<img src=\"./assets/load_extension.png\" width=250 height=250 >\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c9750-8fcc-474b-87c5-ba28448b632d",
   "metadata": {},
   "source": [
    "### Pin the extension\n",
    "Pin your extension to the toolbar to quickly access your extension.\n",
    "\n",
    "<img src=\"./assets/pin_extension.png\" height=250 width=250>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191f358-ebcb-4ca9-b0a5-c80f96ac93fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Code Sample Structure\n",
    "Browser plugin code has two parts, one is backend folder & the other is extension folder.\n",
    "- **Backend** - In the backend folder, we have two python files `code.py` and `server.py`\n",
    "  - `code.py` manages data pre-processing tasks\n",
    "  - `server.py` manages flask server-side operations\n",
    "- **Extension** - In the extension we have the front end code required for the browser plugin (popup.html, popup.js, style.css, manifest.json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d060a9e-0355-41e5-a33c-c8baf35d7481",
   "metadata": {},
   "source": [
    "## Backend code for Text Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de840a1b-5913-455c-bb2e-fe90f4ebb545",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1333e1-1785-4a4b-b426-a7d32640236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.intel import OVModelForCausalLM\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43d5d3-88bf-4bd6-94bd-17c8ffb9006f",
   "metadata": {},
   "source": [
    "### Prompt Templates for Summarization & Question Answering Bot\n",
    "Here we have declared two variables for prompt templates so that it can be called later on , one template for summarization and one for query asked in the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8469f9-5b72-4a2c-bc09-a4406a34a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt template for summarization\n",
    "summary_template= \"\"\"Write a concise summary of the following: \"{context}\" CONCISE SUMMARY: \"\"\"\n",
    "#prompt template for query\n",
    "query_template=\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use 10 words maximum and keep the answer as concise as possible in one sentence.\n",
    "    Always say \"thanks for asking!\" at the end of the answer.\n",
    " \n",
    "    {context}\n",
    " \n",
    "    Question: {question}\n",
    " \n",
    "    Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8958c7f-7241-4784-a6a9-0d59431b0e4e",
   "metadata": {},
   "source": [
    "### Pre-process the Input File\n",
    "\n",
    "* Through browser plugin, users post an input file for summarization. \n",
    "* Document loaders, i.e. WebBaseLoader & PyPDFLoader, in RAG are used to load page content from any Webpage/PDF and preprocess the documents that will be further used for retrieval during the summarization & question answering process.\n",
    "* The loaded page data would be split using Recursive Character Text Splitter & embeddings are created using HuggingFace Embeddings. Here, RecursiveCharacterTextSplitter is used to split text into smaller pieces recursively at the character level.\n",
    "* In RAG, embeddings plays a crucial role in retrieval of relevant documents for a given query and Sentence Transformers helps to generate embeddings for each document in your knowledge base.\n",
    "* These embeddings are further stored into ChromaDB for further retrieval usage. Chroma is a vector store and embeddings database designed from the ground-up to make it easy to build AI applications with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83e322-d97f-420d-88cf-21ceba6261fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(loader):\n",
    "    \"\"\"\n",
    "        This function does the below steps in a sequential order:\n",
    "        1. Loads page content from the webpage/PDF \n",
    "        2. Splits the page data using Recursive Character Text Splitter & creates embeddings using HuggingFace Embeddings\n",
    "        3. This is further stored into ChromaDB for futher retrieval usage\n",
    "        input: loader contains page data from a Webpage/PDF\n",
    "        output: returns a vectorstore\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page_data = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "        all_splits = text_splitter.split_documents(page_data)\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        global vectorstore\n",
    "        vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)  \n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(\"Error while processing Webpage/PDF page content\\n\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0f347-df38-4c93-a77c-ab978c00cfaf",
   "metadata": {},
   "source": [
    "### Load LLM models\n",
    "Below module:\n",
    "1. Fetches the OpenVINO converted model & compiles it on GPU\n",
    "2. Generate a HuggingFace Pipeline for Text-Generation.\n",
    "3. Returns the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750172c-0d22-4e70-98d5-7a5ec29f4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_id):\n",
    "    \"\"\"\n",
    "        Meta Llama2 & Qwen 7B models are converted to OpenVINO IR Format. This function compiles those converted models on GPU.\n",
    "        input: user selected model_id from plugin\n",
    "        output: compiled model with openvino\n",
    "    \"\"\"\n",
    "    if model_id:\n",
    "        try:\n",
    "            if model_id==\"Meta LLama 2\":\n",
    "                model_path=r\"models\\ov_llama_2\"\n",
    "            elif model_id==\"Qwen 7B Instruct\":\n",
    "                model_path=r\"models\\ov_qwen7b\"\n",
    "            model = OVModelForCausalLM.from_pretrained(model_path , device='GPU')\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            pipe=pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=4096,  \n",
    "                device=model.device\n",
    "            )\n",
    "            global llm_model \n",
    "            llm_model = HuggingFacePipeline(pipeline=pipe)\n",
    "            return llm_model\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load the model. Please check whether the model_path is correct\\n\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f8da99-d624-4ca9-aa38-f7fb6f7c3200",
   "metadata": {},
   "source": [
    "###  URL Summarization\n",
    "For a URL Summarization, we load the web page content when end-user enters a URL into the plugin using **WebBaseLoader** which in return loads the page data and passes into the RetrievalQA chain. When a question is being asked in the retreival QA chain , we try to get a concise summary and return it . Here we are using WebBaseLoader to load the documents from the web.\n",
    "* The **WebBaseLoader** in Retrieval Augmented Generation (RAG) is a type of document loader that is designed to load documents from the web.The WebBaseLoader is used when the documents for retrieval are not stored locally or in a Hugging Face dataset, but are instead located on the web.\n",
    "* **RetrievalQA** is a type of question answering system that uses a retriever to fetch relevant documents given a question, and then uses a reader to extract the answer from the retrieved documents.\n",
    "\n",
    "Below module:\n",
    "1. Loads the page data from a webpage using WebBaseLoader.\n",
    "2. Pre-processed the data & stores into a vector store.\n",
    "3. Passes the prompt, vectorstore & LLM model into the chain & returns the summary to the plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebde7e-f6df-4123-88e8-13f9f072d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_url_data(urls):\n",
    "    \"\"\"\n",
    "        When an end user pastes a URL into the plugin, The RAW data is passed onto the RetrievalQA chain,\n",
    "        and the output is returned back to the plugin.\n",
    "        input: Webpage URL(str).\n",
    "        output: Glance Summary of the fetched URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = WebBaseLoader(urls)\n",
    "        global summ_vectorstore \n",
    "        summ_vectorstore = pre_processing(loader) # Common Helper function for processing data.\n",
    "        prompt = PromptTemplate(\n",
    "            template=summary_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "    \n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm_model,\n",
    "            retriever=summ_vectorstore.as_retriever(),\n",
    "            chain_type=\"stuff\",\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=False,\n",
    "        )\n",
    "        \n",
    "        question = \"Please summarize the entire book in one paragraph of 100 words\"\n",
    "        summary = qa_chain(question)\n",
    "        response = summary['result']\n",
    "        summary_start = response.find(\"CONCISE SUMMARY:\")\n",
    "        concise_summary = response[summary_start + len(\"CONCISE SUMMARY:\"):].strip()\n",
    "        return concise_summary\n",
    "    except Exception as e:\n",
    "        print(\"Failed to summarize webpage\\n\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe1bad-94b8-4afd-a906-235a532affb3",
   "metadata": {},
   "source": [
    "### URL Question Answering BOT\n",
    "The below module:\n",
    "1. Fetches any follow up questions related to the summary post URL summarization.\n",
    "2. Passes the **query template** which is declared as global into the prompt, vectorstore, compiled model to the chain.\n",
    "3. When a question is being passed to the retrieval QA chain, the chain loads & posts a precise answer in a single sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad4d92-c524-450b-b8af-e5630ed0b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_on_url_summarized_text(query):\n",
    "    \"\"\"\n",
    "        This function fetches the query asked by the users post summarization from the URL, searches an answer from the vectorstore & returns answer in less than 10 words.\n",
    "        input: user's follow-up question(str)\n",
    "        output: Answer to the conversations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = PromptTemplate(\n",
    "            template=query_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "        reduce_chain = RetrievalQA.from_chain_type(\n",
    "                llm=llm_model,\n",
    "                retriever=summ_vectorstore.as_retriever(),\n",
    "                chain_type=\"stuff\",\n",
    "                chain_type_kwargs={\"prompt\": prompt},\n",
    "                return_source_documents=False\n",
    "            )\n",
    "        summary = reduce_chain({'query': query})\n",
    "        summ_vectorstore.delete\n",
    "        response = summary['result']\n",
    "        summary_start = response.find(\"Helpful Answer:\")\n",
    "        concise_summary = response[summary_start + len(\"Helpful Answer:\"):].strip()\n",
    "        return concise_summary\n",
    "    except Exception as e:\n",
    "        print(\"Error in Webpage Summarizer QA BoT\\n\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401468ef-6d4c-48e2-832f-3e8886aa38c8",
   "metadata": {},
   "source": [
    "### PDF Summarization\n",
    "\n",
    "When end-users upload any PDF file to the plugin, page data is loaded, using **PyPDFLoader** and passed into the RetrievalQA chain for generating a concise summary. When a question is being asked post summarization, a precise answer is returned. \n",
    "\n",
    "* **PyPDFLoader** is a document loader within the LangChain framework specifically designed to handle PDF files. It allows you to extract text from PDF documents and load them into a format suitable for language models and other text-based applications.\n",
    "\n",
    "Below module: \n",
    "1. Loads the page data from a webpage using PyPDFLoader.\n",
    "2. Pre-processed the data & stores into a vector store.\n",
    "3. Passes the prompt, vectorstore & LLM model into the chain & returns the summary to the plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc83ad-3e4a-4d4b-906e-01bbebd10ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_pdf_data(pdf):\n",
    "    \"\"\"\n",
    "        When an end user uploads a PDF into the plugin, The RAW data is passed onto the RetrievalQA chain,\n",
    "        and the output is returned back to the plugin.\n",
    "        input: PDF path(str).\n",
    "        output: Glance Summary of the uploaded PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf, extract_images=False)\n",
    "        global pdf_vectorstore\n",
    "        pdf_vectorstore=pre_processing(loader)\n",
    "    \n",
    "        prompt = PromptTemplate(\n",
    "            template=summary_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        reduce_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm_model,\n",
    "            retriever=pdf_vectorstore.as_retriever(),\n",
    "            chain_type=\"stuff\",\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=False,\n",
    "        )\n",
    "        question = \"Please summarize the entire book in 100 words.\"\n",
    "        summary = reduce_chain({'query': question})\n",
    "\n",
    "        response = summary['result']\n",
    "        summary_start = response.find(\"CONCISE SUMMARY:\")\n",
    "        concise_summary = response[summary_start + len(\"CONCISE SUMMARY:\"):].strip()\n",
    "        return concise_summary\n",
    "    except Exception as e:\n",
    "        print(\"Failed to summarize PDF \\n\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c21bf-4e16-432b-a7e1-7ca72b690020",
   "metadata": {},
   "source": [
    "### PDF Question Answering BOT\n",
    "The below module:\n",
    "1. Posts follow up questions, asked by end-users, related to the summary PDF post PDF summarization\n",
    "2. Passes the **query template** which is declared as global into the prompt, vectorstore, compiled model to the chain.\n",
    "3. When a question is being passed to the retrieval QA chain, the chain loads & posts a precise answer in a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5d11d-0e12-4e1e-9669-31692af50159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_on_pdf_summarized_text(query):\n",
    "    \"\"\"\n",
    "        This function fetches the query asked by the users post summarization from the PDF, then after it searches an answer from the vectorstore & returns answer in less than 10 words.\n",
    "        input: user's follow-up question(str)\n",
    "        output: Answer to the conversations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = PromptTemplate(\n",
    "            template=query_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "        reduce_chain = RetrievalQA.from_chain_type(\n",
    "                llm=llm_model,\n",
    "                retriever=pdf_vectorstore.as_retriever(),\n",
    "                chain_type=\"stuff\",\n",
    "                chain_type_kwargs={\"prompt\": prompt},\n",
    "                return_source_documents=False\n",
    "            )\n",
    "        summary = reduce_chain({'query': query})\n",
    "        response = summary['result']\n",
    "        summary_start = response.find(\"Helpful Answer:\")\n",
    "        concise_summary = response[summary_start + len(\"Helpful Answer:\"):].strip()\n",
    "        return concise_summary\n",
    "    except Exception as e:\n",
    "        print(\"Error in PDF Summarizer QA BoT\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325c0ec-aeb7-492c-8396-0cc5a4c21cb9",
   "metadata": {},
   "source": [
    "## Server-side code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060492e-e311-4bf1-ad3c-1c31c55bda93",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434779c-1d0b-49ec-bbb9-dbbd827f61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from flask import Flask, Response, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import tempfile\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb527c-f34f-4d1d-92f9-6d7bdad50dee",
   "metadata": {},
   "source": [
    "### Initializing the flask app and enabling CORS\n",
    "Here we are initializing a flask and enabling CORS which allows the flask app tobe accessed and interacted with from other domains and we are restricting the types of files that can be uploaded to the application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54773857-d296-44c4-847e-d26da86b01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)  # This will enable CORS for all routes\n",
    "ALLOWED_EXTENSIONS = {'txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72ba5f-b056-4601-a0fc-d963c0cc36ad",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "The below module:\n",
    "1. Fetches the model selected by the end-user through model_id.\n",
    "2. Loads the model which would further trigger the model compilation function present in the main code for summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d0e26-8719-4b54-888d-0264c0954416",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/select-model', methods=['POST'])\n",
    "def select_model():\n",
    "    \"\"\"\n",
    "        Model selection function which would further trigger Model compilation function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        global current_model\n",
    "        data = request.get_json()\n",
    "        model_id = data.get('model_id')\n",
    "        current_model = load_llm(model_id)\n",
    "        return jsonify({'message': f'Model {model_id} loaded successfully.'}), 200\n",
    "    \n",
    "    except Exception:\n",
    "        return jsonify({'message': f'Failed to load model'}), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69e9b5-ddf7-470f-ad93-cffea37f793a",
   "metadata": {},
   "source": [
    "### Yielding Summary onto the plugin\n",
    "This is a generator function which helps to Stream and yield the response content chunk by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f664170-bf7c-41b5-b41b-45217a5c1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_output(process_function, *args):\n",
    "    \"\"\"\n",
    "        Generator function to stream output from a process function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for chunk in process_function(*args):\n",
    "            if chunk is not None:\n",
    "                yield f\"{chunk}\"\n",
    "    except Exception:\n",
    "        yield f\"Error while streaming output\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789173e-f5f3-4c47-b576-d167a70a21b0",
   "metadata": {},
   "source": [
    "### URL processing code\n",
    "This will fetch the URL from the user's input from the plugin and trigger the URL summarization function present in the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36575ff6-f3ee-4ddc-8c56-689c8b6844bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/process-url', methods=['POST'])\n",
    "def process_url():\n",
    "    \"\"\"\n",
    "        Fetches URL from the plugin & triggers the URL summarization function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        url = data.get('url')  \n",
    "        if not url:\n",
    "            return jsonify({'message': 'No URL provided'}), 400\n",
    "        chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "        return Response(stream_output(pre_process_url_data, [url]), content_type='text/event-stream')\n",
    "    \n",
    "    except Exception: \n",
    "        return jsonify({'message': f'Error while processing URL'}), 400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833feed5-8e31-4c1f-8bb2-598322208755",
   "metadata": {},
   "source": [
    "### PDF processing code\n",
    "This function takes the PDF uploaded by the user and trigger the PDF summarization function present in the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9ce78-c713-4204-ad68-9b9a9e9a79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/upload-pdf', methods=['POST'])\n",
    "def upload_pdf():\n",
    "    \"\"\"\n",
    "        Once the PDF's uploaded, the PDF Summarization function's triggered.\n",
    "    \"\"\"\n",
    "   \n",
    "    pdf_file = request.files['pdf'] \n",
    "    if pdf_file and pdf_file.content_type == 'application/pdf':\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_pdf:\n",
    "                pdf_file.save(temp_pdf.name)\n",
    "                temp_pdf_path = temp_pdf.name\n",
    "                print(temp_pdf_path)\n",
    "           \n",
    "            chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "            return Response(stream_output(pre_process_pdf_data, temp_pdf_path), content_type='text/event-stream')\n",
    " \n",
    "        except Exception:\n",
    "            return jsonify({\"message\": f\"Error processing PDF:\"}), 500\n",
    " \n",
    "    else:\n",
    "        return jsonify({\"message\": \"Invalid file type. Please upload a PDF.\"}), 400\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56256d03-048c-4dfe-ae5f-2e7b53f0aa66",
   "metadata": {},
   "source": [
    "### PDF Query code for Question Answering Bot\n",
    "Once the PDF content summarization is done user asks query to the Question Answering bot which gets triggered to the Query for the PDF function present in the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f6d9e-9007-461c-88ed-13260d5cf559",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/your_query_pdf', methods=['POST'])\n",
    "def pdf_process_query():\n",
    "    \"\"\"\n",
    "        This function triggers the PDF Question Answering Bot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        query=data.get('query')\n",
    "        if not data:\n",
    "            return jsonify({'message':'no query provided'}),400\n",
    "        response_message=str(qa_on_pdf_summarized_text(query))\n",
    "        return jsonify({'message': response_message}), 200\n",
    "    except Exception:\n",
    "        return jsonify({'message': f'Error while PDF QA Bot'}), 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d6199-fafa-4340-9c08-af63b23f15a2",
   "metadata": {},
   "source": [
    "### URL Query code for Question Answering Bot\n",
    "Once the URL content summarization is done user asks query to the Question Answering bot which gets triggered to the Query for the URL function present in the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873be699-2bf8-4238-88cd-f3c36431fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/your_query_url', methods=['POST'])\n",
    "def url_process_query():\n",
    "    \"\"\"\n",
    "        This function triggers the URL question answering Bot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        query=data.get('query')\n",
    "        if not data:\n",
    "            return jsonify({'message':'no query provided'}),400\n",
    "        response_message=str(qa_on_url_summarized_text(query))\n",
    "        return jsonify({'message': response_message}), 200\n",
    "    except Exception:\n",
    "        return jsonify({'message': f'Error while URL QA Bot'}), 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1be65-2d3b-4fce-bb31-baa7a6573e25",
   "metadata": {},
   "source": [
    "### Calling the main function\n",
    "This code snippet ensures that the Flask development server starts only when the application is run directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138fa1d-5f11-4629-a636-f17cc114644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run(port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84aa0b-6e95-460c-a55b-f6d4938992ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_plugin",
   "language": "python",
   "name": "summarizer_plugin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
