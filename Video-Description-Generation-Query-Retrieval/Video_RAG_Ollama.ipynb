{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79fcc40b-974c-4d09-9cb4-99f0b6706f33",
   "metadata": {},
   "source": [
    "# Video Description Generation and Query Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae88b40",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a593030",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to generate video descriptions using the **Qwen2.5-VL (Qwen 2.5 Vision-Language model)** via **Ollama** and store their embeddings in **ChromaDB** for efficient semantic search on **Intel® Core™ Ultra Processors**. \n",
    "\n",
    "For each video, a description is generated using Ollama's vision model and stored as an embedding in ChromaDB. When a user submits a query, cosine similarity search is performed in ChromaDB to retrieve the most relevant video description. The matching video is then displayed inline.\n",
    "\n",
    "This sample uses the videos from the [**stepfun-ai/Step-Video-T2V-Eval**](https://huggingface.co/datasets/stepfun-ai/Step-Video-T2V-Eval) Hugging Face dataset.\n",
    "\n",
    "\n",
    "- Uses Ollama as the GPU backend\n",
    "- Simpler setup - no complex model loading required\n",
    "- Uses Qwen2.5-VL vision model through Ollama\n",
    "- ChromaDB and semantic search functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ec879",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274819d8",
   "metadata": {},
   "source": [
    "- During the initial data load, videos from the dataset are processed using **Ollama's Qwen2.5-VL vision model**\n",
    "- The model generates descriptions for each video\n",
    "- Generated video descriptions are converted into embeddings using **Sentence Transformers** (all-MiniLM-L6-v2 model)\n",
    "- These embeddings, along with descriptions and video metadata, are stored in a persistent local **ChromaDB** collection\n",
    "- When a user submits a query, the text is encoded into an embedding and used to perform semantic search (via cosine similarity) over the ChromaDB collection\n",
    "- The most relevant video description and associated video file are returned and displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c8809",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "1. **Ollama installed** and running locally\n",
    "2. **Qwen2.5-VL vision model** pulled in Ollama: `ollama pull llava` or `ollama pull llama3.2-vision` (or any vision-capable model)\n",
    "3. The video dataset downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee3aa8-d8c6-44be-869b-422cc728d452",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies\n",
    "\n",
    "Run this cell first to ensure all required packages are installed in the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install dependencies if not already installed\n",
    "required_packages = [\n",
    "    \"ollama>=0.4.0\",\n",
    "    \"chromadb>=1.0.12\",\n",
    "    \"sentence-transformers>=4.1.0\",\n",
    "    \"opencv-python>=4.8.0\",\n",
    "    \"numpy>=1.24.0\",\n",
    "    \"tqdm>=4.65.0\",\n",
    "]\n",
    "\n",
    "print(\"Checking and installing required packages...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        package_name = package.split(\">=\")[0]\n",
    "        __import__(package_name.replace(\"-\", \"_\"))\n",
    "        print(f\"✓ {package_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✓ {package} installed successfully\")\n",
    "\n",
    "print(\"\\n✅ All dependencies are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8bc613",
   "metadata": {},
   "source": [
    "## Building Ollama with GPU Support (Vulkan)\n",
    "\n",
    "For advanced users who want to build Ollama from source with Vulkan GPU acceleration on Windows:\n",
    "\n",
    "### Installation Steps\n",
    "\n",
    "1. **Install Vulkan SDK**\n",
    "   - Download from: https://vulkan.lunarg.com/sdk/home\n",
    "\n",
    "2. **Install TDM-GCC**\n",
    "   - Download from: https://github.com/jmeubank/tdm-gcc/releases/tag/v10.3.0-tdm64-2\n",
    "\n",
    "3. **Install Go SDK**\n",
    "   - Download Go v1.24.9: https://go.dev/dl/go1.24.9.windows-amd64.msi\n",
    "\n",
    "4. **Build Ollama**\n",
    "   ```bash\n",
    "   # Set environment variables\n",
    "   set CGO_ENABLED=1\n",
    "   set CGO_CFLAGS=-IC:\\VulkanSDK\\1.4.321.1\\Include\n",
    "   \n",
    "   # Build with CMake\n",
    "   cmake -B build\n",
    "   cmake --build build --config Release -j14\n",
    "   \n",
    "   # Build Go binary\n",
    "   go build\n",
    "   \n",
    "   # Run Ollama server (Terminal 1)\n",
    "   go run . serve\n",
    "   \n",
    "   # Test with a model (Terminal 2)\n",
    "   ollama run gemma3:270m\n",
    "   ```\n",
    "\n",
    "**Note:** This is for advanced users who want to compile Ollama from source. The pre-built Ollama installation works fine for most users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f262ea",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import chromadb\n",
    "import warnings\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Video, display\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f3cf3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600359d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama configuration\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "VISION_MODEL = \"llama3.2-vision\"  # or \"llava\" or other vision models\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Database configuration\n",
    "DATABASE_PATH = \"./Video_descriptions_database_ollama\"\n",
    "COLLECTION_NAME = \"Video_descriptions_ollama\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc20fa8",
   "metadata": {},
   "source": [
    "## Get video file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_paths():\n",
    "    \"\"\"\n",
    "    Select the number of videos to process and return the selected video file paths.\n",
    "\n",
    "    Returns:\n",
    "        list: Selected list of video files paths.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset_folder = \"Step-Video-T2V-Eval\"\n",
    "        max_videos_to_select = 128\n",
    "        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']\n",
    "        video_files = []\n",
    "        \n",
    "        for root, dirs, files in os.walk(dataset_folder):\n",
    "            video_files.extend([\n",
    "                os.path.join(root, f) for f in files \n",
    "                if any(f.lower().endswith(ext) for ext in video_extensions)\n",
    "            ])\n",
    "        \n",
    "        total_video_files = len(video_files)\n",
    "        num_videos_to_select = min(total_video_files, max_videos_to_select)\n",
    "        \n",
    "        random.seed(42)\n",
    "        selected_video_files = random.sample(video_files, num_videos_to_select)\n",
    "        \n",
    "        logging.info(f\" Total number of video files found: {total_video_files}\")\n",
    "        logging.info(f\" Selected {num_videos_to_select} video files\")\n",
    "        \n",
    "        return selected_video_files\n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while extracting the video paths: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef968d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_video_files = get_video_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541e91c",
   "metadata": {},
   "source": [
    "## Initialize models\n",
    "Initialize the Sentence Transformer model for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c252d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding_model():\n",
    "    \"\"\"\n",
    "    Initialize Sentence Transformer model for generating embeddings.\n",
    "\n",
    "    Returns:\n",
    "        SentenceTransformer: The initialized embedding model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\" Loading Sentence Transformer Model: {EMBEDDING_MODEL}\")\n",
    "        embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "        return embedding_model\n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while loading the embedding model: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = initialize_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fe5a1",
   "metadata": {},
   "source": [
    "## Encode video frame to base64\n",
    "Utility function to encode video frames for Ollama API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_video_frame(video_path, frame_time=0):\n",
    "    \"\"\"\n",
    "    Extract and encode a frame from the video as base64.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        frame_time (float): Time in seconds to extract the frame.\n",
    "    \n",
    "    Returns:\n",
    "        str: Base64 encoded image string or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import cv2\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            logging.error(f\" Cannot open video: {video_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Set position to specific time\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_number = int(frame_time * fps)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret:\n",
    "            logging.error(f\" Cannot read frame from video: {video_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Encode frame to base64\n",
    "        _, buffer = cv2.imencode('.jpg', frame)\n",
    "        frame_base64 = base64.b64encode(buffer).decode('utf-8')\n",
    "        \n",
    "        return frame_base64\n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error encoding video frame: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce50cd",
   "metadata": {},
   "source": [
    "## Generate video description using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb851d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video_description_ollama(video_path):\n",
    "    \"\"\"\n",
    "    Generate video description using Ollama vision model.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated description of the video.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract a frame from the middle of the video\n",
    "        frame_base64 = encode_video_frame(video_path, frame_time=2.0)\n",
    "        \n",
    "        if not frame_base64:\n",
    "            return \"Unable to process video\"\n",
    "        \n",
    "        # Use Ollama API to generate description\n",
    "        response = ollama.chat(\n",
    "            model=VISION_MODEL,\n",
    "            messages=[{\n",
    "                'role': 'user',\n",
    "                'content': 'Describe this sports video in detail. Focus on the main activity, people, objects, and setting.',\n",
    "                'images': [frame_base64]\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        description = response['message']['content']\n",
    "        return description\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error generating description with Ollama: {str(e)}\")\n",
    "        return \"Error generating description\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381f556",
   "metadata": {},
   "source": [
    "## Get or create ChromaDB collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7cdef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_database():\n",
    "    \"\"\"\n",
    "    Connects to or creates a persistent ChromaDB collection.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (collection, existing_descriptions)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=DATABASE_PATH)\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=COLLECTION_NAME,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        logging.info(\" Checking existing descriptions in database...\")\n",
    "        all_items = collection.get(include=[\"metadatas\", \"documents\"])\n",
    "        \n",
    "        existing_descriptions = {}\n",
    "        for metadata, doc in zip(all_items['metadatas'], all_items['documents']):\n",
    "            existing_descriptions[metadata['video_filename']] = doc\n",
    "        \n",
    "        logging.info(f\" Found {len(existing_descriptions)} existing descriptions\")\n",
    "        return collection, existing_descriptions\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while checking database: {str(e)}\")\n",
    "        return None, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection, existing_descriptions = get_or_create_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438c801",
   "metadata": {},
   "source": [
    "## Generate and store video descriptions\n",
    "\n",
    "Each video will be processed:\n",
    "1. Check if description already exists in database\n",
    "2. If not, generate description using Ollama\n",
    "3. Create embedding and store in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e69a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_store_video_descriptions(selected_video_files, collection, existing_descriptions, embedding_model):\n",
    "    \"\"\"\n",
    "    Generate and store video descriptions using Ollama.\n",
    "    \n",
    "    Args:\n",
    "        selected_video_files (list): List of video file paths.\n",
    "        collection: ChromaDB collection object.\n",
    "        existing_descriptions (dict): Already processed videos.\n",
    "        embedding_model: Sentence Transformer model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        video_descriptions = {}\n",
    "        \n",
    "        for video_file in tqdm(selected_video_files, desc=\"Processing videos\"):\n",
    "            video_filename = os.path.basename(video_file)\n",
    "            \n",
    "            # Skip if already processed\n",
    "            if video_filename in existing_descriptions:\n",
    "                logging.info(f\" Skipping {video_filename} - already in database\")\n",
    "                video_descriptions[video_file] = existing_descriptions[video_filename]\n",
    "                continue\n",
    "            \n",
    "            logging.info(f\"\\n Processing {video_filename} using Ollama...\")\n",
    "            \n",
    "            # Generate description using Ollama\n",
    "            description_text = generate_video_description_ollama(video_file)\n",
    "            video_descriptions[video_file] = description_text\n",
    "            \n",
    "            logging.info(f\" Generated description: {description_text}\\n\")\n",
    "            \n",
    "            # Create embedding\n",
    "            embedding = embedding_model.encode(description_text).tolist()\n",
    "            \n",
    "            # Store in ChromaDB\n",
    "            collection.add(\n",
    "                embeddings=[embedding],\n",
    "                documents=[description_text],\n",
    "                metadatas=[{\"video_filename\": video_filename}],\n",
    "                ids=[video_file]\n",
    "            )\n",
    "            \n",
    "            logging.info(f\" Added {video_filename} to database\\n\")\n",
    "        \n",
    "        logging.info(f\"\\n Processed {len(video_descriptions)} videos\")\n",
    "        logging.info(f\" Database now has {collection.count()} total descriptions\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while generating and storing descriptions: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2a1ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "  Generating video descriptions using Ollama. This may take some time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1290f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_store_video_descriptions(selected_video_files, collection, existing_descriptions, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8b66c",
   "metadata": {},
   "source": [
    "## Query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d801454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_videos_descriptions(query, collection, embedding_model):\n",
    "    \"\"\"\n",
    "    Query ChromaDB collection to find similar videos.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query.\n",
    "        collection: ChromaDB collection object.\n",
    "        embedding_model: Sentence Transformer model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Query results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embedding = embedding_model.encode(query).tolist()\n",
    "        \n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=1,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        logging.info(f\" Search results for: '{query}'\\n\")\n",
    "        \n",
    "        for doc, metadata, distance in zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        ):\n",
    "            similarity_score = 1 - distance\n",
    "            logging.info(f\" Video filename: {metadata['video_filename']}\")\n",
    "            logging.info(f\" Similarity score: {similarity_score:.3f}\")\n",
    "            logging.info(f\" Distance: {distance:.3f}\")\n",
    "            logging.info(f\" Video description: {doc}\\n\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while querying video descriptions: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed608765",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me the video of the birds and blue sea\"\n",
    "results = query_videos_descriptions(query, collection, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399bfd5",
   "metadata": {},
   "source": [
    "## Display the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video(results):\n",
    "    \"\"\"\n",
    "    Display the video based on query results.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Query results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if results and results['ids']:\n",
    "            video_path = results['ids'][0][0]\n",
    "            video = Video(video_path, width=600, height=400)\n",
    "            display(video)\n",
    "        else:\n",
    "            logging.info(\" No video found\")\n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while displaying the video: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf235da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2e217",
   "metadata": {},
   "source": [
    "## Remove the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ba65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_database():\n",
    "    \"\"\"\n",
    "    Delete the database directory.\n",
    "    \"\"\"\n",
    "    if os.path.exists(DATABASE_PATH):\n",
    "        logging.info(\"Database deletion option available.\")\n",
    "        database_deletion = 'no'  # Change to 'yes' to delete\n",
    "        \n",
    "        if database_deletion == 'yes':\n",
    "            try:\n",
    "                shutil.rmtree(DATABASE_PATH)\n",
    "                logging.info(\" Database deleted!\")\n",
    "            except Exception as e:\n",
    "                logging.exception(f\" Error while deleting database: {str(e)}\")\n",
    "        else:\n",
    "            logging.info(\" Database not deleted\")\n",
    "    else:\n",
    "        logging.info(\" Database is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8070cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2bba50",
   "metadata": {},
   "source": [
    "## Dataset Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613a434",
   "metadata": {},
   "source": [
    "    @misc{ma2025stepvideot2vtechnicalreportpractice,  \n",
    "      title={Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model}, \n",
    "      author={Guoqing Ma and Haoyang Huang and Kun Yan and Liangyu Chen and Nan Duan and Shengming Yin and Changyi Wan and Ranchen Ming and Xiaoniu Song and Xing Chen and Yu Zhou and Deshan Sun and Deyu Zhou and Jian Zhou and Kaijun Tan and Kang An and Mei Chen and Wei Ji and Qiling Wu and Wen Sun and Xin Han and Yanan Wei and Zheng Ge and Aojie Li and Bin Wang and Bizhu Huang and Bo Wang and Brian Li and Changxing Miao and Chen Xu and Chenfei Wu and Chenguang Yu and Dapeng Shi and Dingyuan Hu and Enle Liu and Gang Yu and Ge Yang and Guanzhe Huang and Gulin Yan and Haiyang Feng and Hao Nie and Haonan Jia and Hanpeng Hu and Hanqi Chen and Haolong Yan and Heng Wang and Hongcheng Guo and Huilin Xiong and Huixin Xiong and Jiahao Gong and Jianchang Wu and Jiaoren Wu and Jie Wu and Jie Yang and Jiashuai Liu and Jiashuo Li and Jingyang Zhang and Junjing Guo and Junzhe Lin and Kaixiang Li and Lei Liu and Lei Xia and Liang Zhao and Liguo Tan and Liwen Huang and Liying Shi and Ming Li and Mingliang Li and Muhua Cheng and Na Wang and Qiaohui Chen and Qinglin He and Qiuyan Liang and Quan Sun and Ran Sun and Rui Wang and Shaoliang Pang and Shiliang Yang and Sitong Liu and Siqi Liu and Shuli Gao and Tiancheng Cao and Tianyu Wang and Weipeng Ming and Wenqing He and Xu Zhao and Xuelin Zhang and Xianfang Zeng and Xiaojia Liu and Xuan Yang and Yaqi Dai and Yanbo Yu and Yang Li and Yineng Deng and Yingming Wang and Yilei Wang and Yuanwei Lu and Yu Chen and Yu Luo and Yuchu Luo and Yuhe Yin and Yuheng Feng and Yuxiang Yang and Zecheng Tang and Zekai Zhang and Zidong Yang and Binxing Jiao and Jiansheng Chen and Jing Li and Shuchang Zhou and Xiangyu Zhang and Xinhao Zhang and Yibo Zhu and Heung-Yeung Shum and Daxin Jiang},\n",
    "      year={2025},\n",
    "      eprint={2502.10248},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CV},\n",
    "      url={https://arxiv.org/abs/2502.10248}, \n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI PC Samples",
   "language": "python",
   "name": "ai_pc_samples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
