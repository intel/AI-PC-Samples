<!doctype html>
<html lang="en">
    <head></head>
    <body>
        <h1 style="text-align: center;">WebLLM Chat Bot</h1>
        <p style="text-align: center;">Streaming the LLM output</p>
        <p style="text-align: center;">Open the console to see the response</p>
        <!-- External JavaScript File -->
        <script type="module">
            // Source: https://webllm.mlc.ai/docs/user/basic_usage.html#streaming-chat-completion
            import * as webllm from "https://esm.run/@mlc-ai/web-llm";

            // Initialize with a progress callback
            const initProgressCallback = (progress) => {
                console.log("Model loading progress:", progress);
            };


            // Using CreateMLCEngine
            const engine = await webllm.CreateMLCEngine("Llama-3.2-1B-Instruct-q4f32_1-MLC", { initProgressCallback });

            const messages = [
                { role: "system", content: "You are a helpful AI assistant." },
                { role: "user", content: "Hello!" }
            ];

            // Chunks is an AsyncGenerator object
            const chunks = await engine.chat.completions.create({
                messages,
                temperature: 1,
                stream: true, // <-- Enable streaming
                stream_options: { include_usage: true },
            });

            let reply = "";
            for await (const chunk of chunks) {
                reply += chunk.choices[0]?.delta.content || "";
                console.log(reply);
                if (chunk.usage) {
                    console.log(chunk.usage); // only last chunk has usage
                }
            }

            const fullReply = await engine.getMessage();
            console.log(fullReply);
        </script>
    </body>
</html>
