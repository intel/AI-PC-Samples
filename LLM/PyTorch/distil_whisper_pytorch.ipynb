{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper Speech Recognition on AI PCs: Local Audio Intelligence\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to run OpenAI's Whisper speech recognition model locally on an AI PC. It is optimized for Intel® Core™ Ultra processors, utilizing the integrated GPU (Intel® Arc™ Graphics) for efficient audio transcription and translation workloads.\n",
    "\n",
    "## What is an AI PC?\n",
    "\n",
    "An AI PC is a next-generation computing platform equipped with a CPU, GPU, and NPU, each designed with specific AI acceleration capabilities.\n",
    "\n",
    "**Fast Response (CPU)**  \n",
    "The central processing unit (CPU) is optimized for smaller, low-latency workloads, making it ideal for quick responses and general-purpose tasks.\n",
    "\n",
    "**High Throughput (GPU)**  \n",
    "The graphics processing unit (GPU) excels at handling large-scale workloads that require high parallelism and throughput, making it suitable for tasks like speech recognition and audio processing.\n",
    "\n",
    "**Power Efficiency (NPU)**  \n",
    "The neural processing unit (NPU) is designed for sustained, heavily-used AI workloads, delivering high efficiency and low power consumption for continuous inference tasks.\n",
    "\n",
    "The AI PC represents a transformative shift in computing, enabling advanced AI applications like Whisper speech recognition to run seamlessly on local hardware. This innovation enhances privacy, reduces latency, and eliminates dependency on cloud services for audio processing.\n",
    "\n",
    "## What is Whisper?\n",
    "\n",
    "Whisper is a state-of-the-art automatic speech recognition (ASR) system developed by OpenAI. It was trained on 680,000 hours of multilingual and multitask supervised data, making it robust to accents, background noise, and technical language.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this workshop, participants will be able to:\n",
    "\n",
    "1. **Remember**: Recall the main components of a speech-to-text (STT) pipeline using Whisper models\n",
    "2. **Understand**: Explain how Whisper models process audio data and generate transcriptions\n",
    "3. **Apply**: Implement a speech transcription application using Whisper models and PyTorch\n",
    "4. **Analyze**: Examine the performance characteristics of Whisper models and identify optimization strategies\n",
    "5. **Evaluate**: Compare different Whisper model configurations and their impact on transcription quality and speed\n",
    "6. **Create**: Develop a custom speech recognition application with streaming output optimized for Intel XPU hardware\n",
    "\n",
    "## Key Features of This Implementation\n",
    "\n",
    "- **Local Processing**: All audio data stays on your device for privacy\n",
    "- **GPU Acceleration**: Utilizes Intel® Arc™ Graphics for fast inference\n",
    "- **Multiple Languages**: Support for English, Spanish, French, German, and Chinese\n",
    "- **Real-time Performance**: Optimized for responsive audio processing\n",
    "- **Memory Efficiency**: Smart caching and memory management for sustained usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "### Code Walkthrough:\n",
    "\n",
    "• **Import Essential Libraries**: PyTorch for deep learning, Transformers for Whisper models, datasets for audio data loading\n",
    "\n",
    "• **Global State Management**: MODEL_CACHE stores loaded models to prevent redundant loading, GENERATION_COUNT tracks inference operations\n",
    "\n",
    "• **Device Detection**: Automatically detects Intel XPU (GPU) availability and sets appropriate data types (FP16 for XPU, FP32 for CPU)\n",
    "\n",
    "• **Memory Management Functions**: \n",
    "  - `clear_gpu_memory()`: Clears XPU cache, synchronizes device, and performs garbage collection\n",
    "  - `reset_model_state()`: Resets model's internal caches and decoder states to prevent memory accumulation\n",
    "\n",
    "• **XPU Optimizations**: When Intel Arc Graphics is detected, uses FP16 precision for 2-3x speedup with minimal accuracy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSpeechSeq2Seq, pipeline, AutoProcessor\n",
    "from transformers.models.whisper import WhisperFeatureExtractor, WhisperTokenizer\n",
    "from IPython.display import display, Audio, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Global model cache to prevent reloading\n",
    "MODEL_CACHE = {}\n",
    "\n",
    "# Track generation count for memory management\n",
    "GENERATION_COUNT = 0\n",
    "MAX_GENERATIONS_BEFORE_RESET = 50\n",
    "\n",
    "# Check if we have access to Intel XPU hardware\n",
    "if hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
    "    device = 'xpu'\n",
    "    dtype = torch.float16\n",
    "    print(f\"Using Intel XPU device: {torch.xpu.get_device_name()}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    dtype = torch.float32\n",
    "    print(\"Using CPU for inference\")\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"\n",
    "    Clear GPU/XPU memory and reset cache.\n",
    "    \n",
    "    This function performs garbage collection and clears GPU memory cache\n",
    "    to free up resources for subsequent operations.\n",
    "    \"\"\"\n",
    "    if device == 'xpu':\n",
    "        torch.xpu.empty_cache()\n",
    "        torch.xpu.synchronize()\n",
    "        if hasattr(torch.xpu, 'reset_peak_memory_stats'):\n",
    "            torch.xpu.reset_peak_memory_stats()\n",
    "    gc.collect()\n",
    "    print(\"GPU memory cleared\")\n",
    "\n",
    "def reset_model_state(model):\n",
    "    \"\"\"\n",
    "    Reset model's internal state and caches.\n",
    "    \n",
    "    Args:\n",
    "        model: The Whisper model to reset\n",
    "        \n",
    "    Returns:\n",
    "        The reset model or None if model is invalid\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "        \n",
    "    if hasattr(model, 'model'):\n",
    "        model = model.model\n",
    "    \n",
    "    # Clear any cached states in encoder/decoder\n",
    "    if hasattr(model, 'encoder'):\n",
    "        if hasattr(model.encoder, 'clear_cache'):\n",
    "            model.encoder.clear_cache()\n",
    "    \n",
    "    if hasattr(model, 'decoder'):\n",
    "        if hasattr(model.decoder, 'clear_cache'):\n",
    "            model.decoder.clear_cache()\n",
    "        model.decoder.past_key_values = None\n",
    "    \n",
    "    # Reset generation config safely\n",
    "    if hasattr(model, 'generation_config') and model.generation_config is not None:\n",
    "        try:\n",
    "            model.generation_config.use_cache = True\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading a Sample Audio Dataset\n",
    "\n",
    "### Code Walkthrough:\n",
    "\n",
    "• **Dataset Loading**: Uses Hugging Face's streaming API to load LibriSpeech dataset without downloading the entire dataset\n",
    "\n",
    "• **Audio Properties**: Extracts audio waveform array and sampling rate (typically 16kHz for speech)\n",
    "\n",
    "• **Error Handling**: Falls back to synthetic audio generation if dataset loading fails (useful for offline testing)\n",
    "\n",
    "• **Audio Playback**: Displays an interactive audio player in the notebook for listening to the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LibriSpeech sample dataset\n",
    "print(\"Loading sample audio from LibriSpeech dataset...\")\n",
    "try:\n",
    "    dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\", streaming=True)\n",
    "    dataset_iter = iter(dataset)\n",
    "    sample_data = next(dataset_iter)\n",
    "    \n",
    "    # Get a sample audio\n",
    "    audio = sample_data[\"audio\"]\n",
    "    print(f\"Audio sample rate: {audio['sampling_rate']} Hz\")\n",
    "    print(f\"Audio duration: {len(audio['array']) / audio['sampling_rate']:.2f} seconds\")\n",
    "    \n",
    "    # Display audio waveform\n",
    "    display(Audio(audio['array'], rate=audio['sampling_rate']))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    # Create synthetic audio as fallback\n",
    "    sr = 16000\n",
    "    duration = 5\n",
    "    t = np.linspace(0, duration, int(sr * duration), False)\n",
    "    audio = {\n",
    "        \"array\": 0.5 * np.sin(2*np.pi*440*t).astype(np.float32),\n",
    "        \"sampling_rate\": sr\n",
    "    }\n",
    "    print(\"Using synthetic audio for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the Whisper Model\n",
    "\n",
    "### Code Walkthrough:\n",
    "\n",
    "• **Model Loading Function**: Comprehensive function that handles model initialization with caching and proper configuration\n",
    "\n",
    "• **Key Components Loaded**:\n",
    "  - **AutoModelForSpeechSeq2Seq**: The main Whisper model architecture for sequence-to-sequence speech recognition\n",
    "  - **WhisperTokenizer**: Converts text to/from token IDs, handles special tokens\n",
    "  - **WhisperFeatureExtractor**: Converts raw audio to mel-spectrogram features\n",
    "  - **AutoProcessor**: Combines feature extractor and tokenizer for streamlined processing\n",
    "\n",
    "• **Model Configuration**:\n",
    "  - Detects English-only vs multilingual models automatically\n",
    "  - Sets appropriate decoder start tokens and forced decoder IDs\n",
    "  - Disables gradient computation for inference efficiency\n",
    "\n",
    "• **Memory Optimization**:\n",
    "  - Uses `low_cpu_mem_usage=True` to reduce RAM usage during loading\n",
    "  - Loads with appropriate dtype (FP16 for XPU, FP32 for CPU)\n",
    "  - Implements model caching to avoid redundant loading\n",
    "\n",
    "• **XPU Optimizations**: When running on Intel Arc Graphics, models are automatically moved to XPU device for acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_whisper_model(model_name=\"distil-whisper/distil-small.en\", language=\"english\", force_reload=False):\n",
    "    \"\"\"\n",
    "    Load Whisper model with proper initialization.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): HuggingFace model identifier\n",
    "        language (str): Language for transcription\n",
    "        force_reload (bool): Force reload even if cached\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing model, tokenizer, processor, and metadata\n",
    "    \"\"\"\n",
    "    global MODEL_CACHE, dtype, device, GENERATION_COUNT\n",
    "    \n",
    "    # Reset generation count when loading new model\n",
    "    GENERATION_COUNT = 0\n",
    "    \n",
    "    # Check if model is already loaded\n",
    "    if not force_reload and model_name in MODEL_CACHE:\n",
    "        print(f\"Using cached model: {model_name}\")\n",
    "        return MODEL_CACHE[model_name]\n",
    "    \n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Clear previous models\n",
    "    if MODEL_CACHE:\n",
    "        print(\"Clearing previous models...\")\n",
    "        for cached_model in list(MODEL_CACHE.keys()):\n",
    "            try:\n",
    "                if 'model' in MODEL_CACHE[cached_model]:\n",
    "                    MODEL_CACHE[cached_model]['model'].cpu()\n",
    "                    del MODEL_CACHE[cached_model]['model']\n",
    "                del MODEL_CACHE[cached_model]\n",
    "            except:\n",
    "                pass\n",
    "        MODEL_CACHE.clear()\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        # Determine if this is an English-only model\n",
    "        is_english_only = any(x in model_name.lower() for x in ['.en', 'english-only'])\n",
    "        print(f\"Model type: {'English-only' if is_english_only else 'Multilingual'}\")\n",
    "        \n",
    "        # Load tokenizer and processor\n",
    "        if is_english_only:\n",
    "            tokenizer = WhisperTokenizer.from_pretrained(model_name)\n",
    "            processor = AutoProcessor.from_pretrained(model_name)\n",
    "        else:\n",
    "            tokenizer = WhisperTokenizer.from_pretrained(model_name, language=language, task=\"transcribe\")\n",
    "            processor = AutoProcessor.from_pretrained(model_name, language=language, task=\"transcribe\")\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True\n",
    "        )\n",
    "        \n",
    "        # Move to device and set to eval mode\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Disable gradient computation\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Configure model\n",
    "        model.config.forced_decoder_ids = None\n",
    "        model.config.suppress_tokens = []\n",
    "        \n",
    "        # Set up decoder start token\n",
    "        if model.config.decoder_start_token_id is None:\n",
    "            if is_english_only:\n",
    "                model.config.decoder_start_token_id = tokenizer.pad_token_id or 50257\n",
    "                model.config.forced_decoder_ids = [\n",
    "                    [1, 50259],  # Start of transcript token\n",
    "                    [2, 50359],  # Transcribe token\n",
    "                    [3, 50363]   # No timestamps token\n",
    "                ]\n",
    "            else:\n",
    "                model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        # Load feature extractor\n",
    "        feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "        \n",
    "        # Ensure tokenizer has proper tokens\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Cache model\n",
    "        MODEL_CACHE[model_name] = {\n",
    "            'model': model,\n",
    "            'feature_extractor': feature_extractor,\n",
    "            'tokenizer': tokenizer,\n",
    "            'processor': processor,\n",
    "            'dtype': dtype,\n",
    "            'is_english_only': is_english_only,\n",
    "            'generation_count': 0\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Model loaded successfully: {model_name}\")\n",
    "        print(f\"   Decoder start token ID: {model.config.decoder_start_token_id}\")\n",
    "        print(f\"   Pad token ID: {tokenizer.pad_token_id}\")\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        return MODEL_CACHE[model_name]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        clear_gpu_memory()\n",
    "        raise\n",
    "\n",
    "# Load the model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Loading Distil-Whisper Small English model...\")\n",
    "print(\"=\"*50)\n",
    "model_components = load_whisper_model(\"distil-whisper/distil-small.en\")\n",
    "\n",
    "# Extract components\n",
    "model = model_components['model']\n",
    "feature_extractor = model_components['feature_extractor']\n",
    "tokenizer = model_components['tokenizer']\n",
    "processor = model_components['processor']\n",
    "\n",
    "print(\"\\nModel ready for transcription!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Transcription with Intel XPU Optimizations\n",
    "\n",
    "### Understanding Intel XPU Acceleration\n",
    "\n",
    "Intel XPU (eXtended Processing Unit) is Intel's unified abstraction for accessing Intel GPUs in PyTorch. When running on Intel® Arc™ Graphics or Intel® Data Center GPUs, XPU provides significant acceleration for deep learning workloads.\n",
    "\n",
    "### Key Optimization Techniques\n",
    "\n",
    "#### 1. **Automatic Mixed Precision (AMP) with torch.autocast**\n",
    "\n",
    "Automatic Mixed Precision allows models to use both FP16 and FP32 computations automatically:\n",
    "- **FP16 operations**: Used where precision loss is acceptable (most computations)  \n",
    "- **FP32 operations**: Maintained for operations requiring high precision  \n",
    "- **Benefits**: 2-3x speedup with minimal accuracy loss\n",
    "\n",
    "Example:\n",
    "```python\n",
    "with torch.autocast(device_type=\"xpu\", dtype=torch.float16, enabled=True):\n",
    "    # Model computations automatically use optimal precision\n",
    "    output = model(input)\n",
    "```\n",
    "\n",
    "#### 2. **Gradient Computation Control with torch.no_grad()**\n",
    "During inference, we disable gradient computation to:\n",
    "\n",
    "- Reduce memory usage by ~50%\n",
    "- Accelerate forward pass computation\n",
    "- Prevent unnecessary gradient accumulation\n",
    "\n",
    "Example:\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # All operations here won't track gradients\n",
    "    output = model(input)\n",
    "```\n",
    "\n",
    "#### 3. **Device Synchronization**\n",
    "Intel XPU operations are asynchronous by default. Synchronization ensures:\n",
    "\n",
    "- All GPU operations complete before CPU continues\n",
    "- Accurate timing measurements\n",
    "- Proper memory cleanup\n",
    "\n",
    "Example:\n",
    "```python\n",
    "torch.xpu.synchronize()  # Wait for all XPU operations to complete\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Walkthrough:\n",
    "\n",
    "• **Streaming Architecture**: Processes audio in chunks (default 10s) for real-time feedback and memory efficiency\n",
    "\n",
    "• **Chunk Processing Pipeline**:\n",
    "  1. Split audio into overlapping chunks\n",
    "  2. Process each chunk through feature extraction\n",
    "  3. Generate transcription with beam search\n",
    "  4. Display intermediate results in real-time\n",
    "\n",
    "• **XPU Optimizations Applied**:\n",
    "  - **torch.autocast**: Automatic mixed precision for 2-3x speedup\n",
    "  - **torch.no_grad()**: Disables gradient tracking for inference\n",
    "  - **torch.xpu.synchronize()**: Ensures proper timing measurements\n",
    "  - **Periodic memory cleanup**: Clears cache every 3 chunks\n",
    "\n",
    "• **Generation Parameters**:\n",
    "  - `max_new_tokens`: Dynamically set based on chunk duration\n",
    "  - `num_beams=1`: Greedy decoding for speed\n",
    "  - `use_cache=False`: Prevents memory accumulation in long streams\n",
    "\n",
    "• **Error Recovery**: Continues processing even if individual chunks fail, ensuring robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_streaming(model_components, audio, chunk_length_s=10, max_chunk_length=20):\n",
    "    \"\"\"\n",
    "    Perform streaming transcription with chunk processing and Intel XPU optimizations.\n",
    "    \n",
    "    Args:\n",
    "        model_components (dict): Dictionary containing model and tokenizer\n",
    "        audio (dict): Audio dictionary with 'array' and 'sampling_rate'\n",
    "        chunk_length_s (int): Length of each chunk in seconds\n",
    "        max_chunk_length (int): Maximum allowed chunk length\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (transcription_text, inference_time)\n",
    "    \"\"\"\n",
    "    global GENERATION_COUNT, MAX_GENERATIONS_BEFORE_RESET\n",
    "    \n",
    "    if not model_components or 'model' not in model_components:\n",
    "        print(\"Invalid model components. Please run Cell 3 to load the model first.\")\n",
    "        return \"\", 0\n",
    "    \n",
    "    model = model_components['model']\n",
    "    processor = model_components['processor']\n",
    "    tokenizer = model_components['tokenizer']\n",
    "    model_dtype = model_components.get('dtype', torch.float32)\n",
    "    is_english_only = model_components.get('is_english_only', False)\n",
    "    \n",
    "    # Get audio data\n",
    "    audio_array = audio[\"array\"]\n",
    "    sampling_rate = audio[\"sampling_rate\"]\n",
    "    \n",
    "    # Limit chunk length\n",
    "    chunk_length_s = min(chunk_length_s, max_chunk_length)\n",
    "    chunk_size_samples = int(chunk_length_s * sampling_rate)\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(audio_array), chunk_size_samples):\n",
    "        chunk = audio_array[i:i+chunk_size_samples]\n",
    "        if len(chunk) >= sampling_rate * 0.5:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    print(f\"Processing {len(chunks)} chunks of {chunk_length_s} seconds\")\n",
    "    print(f\"Model: {'English-only' if is_english_only else 'Multilingual'}\")\n",
    "    print(f\"Device: {device}, Dtype: {model_dtype}\")\n",
    "    \n",
    "    full_transcription = \"\"\n",
    "    start_time = time.time()\n",
    "    successful_chunks = 0\n",
    "    \n",
    "    # Use autocast for XPU optimization\n",
    "    autocast_dtype = model_dtype if device == 'xpu' else torch.float32\n",
    "    use_autocast = device == 'xpu' and model_dtype in [torch.float16, torch.bfloat16]\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "            \n",
    "            # Process chunk\n",
    "            inputs = processor(chunk, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "            \n",
    "            # Move input features to device with proper dtype\n",
    "            input_features = inputs.input_features.to(device=device, dtype=model_dtype)\n",
    "            \n",
    "            # Generate with proper initialization and XPU optimizations\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device, dtype=autocast_dtype, enabled=use_autocast):\n",
    "                    chunk_duration = len(chunk) / sampling_rate\n",
    "                    max_tokens = min(100, int(chunk_duration * 20))\n",
    "                    \n",
    "                    # Base generation kwargs\n",
    "                    generate_kwargs = {\n",
    "                        \"inputs\": input_features,\n",
    "                        \"max_new_tokens\": max_tokens,\n",
    "                        \"num_beams\": 1,\n",
    "                        \"do_sample\": False,\n",
    "                        \"use_cache\": False,\n",
    "                        \"return_dict_in_generate\": False,\n",
    "                    }\n",
    "                    \n",
    "                    # Add decoder_start_token_id if not set\n",
    "                    if model.config.decoder_start_token_id is not None:\n",
    "                        generate_kwargs[\"decoder_start_token_id\"] = model.config.decoder_start_token_id\n",
    "                    \n",
    "                    # Handle forced_decoder_ids\n",
    "                    if hasattr(model.config, 'forced_decoder_ids') and model.config.forced_decoder_ids:\n",
    "                        generate_kwargs[\"forced_decoder_ids\"] = model.config.forced_decoder_ids\n",
    "                    \n",
    "                    try:\n",
    "                        # Try generation\n",
    "                        generated_ids = model.generate(**generate_kwargs)\n",
    "                    except Exception as e:\n",
    "                        if \"index\" in str(e) and \"out of bounds\" in str(e):\n",
    "                            print(f\"Retrying with explicit decoder_input_ids...\")\n",
    "                            decoder_input_ids = torch.tensor(\n",
    "                                [[model.config.decoder_start_token_id or tokenizer.pad_token_id]], \n",
    "                                device=device\n",
    "                            )\n",
    "                            generate_kwargs[\"decoder_input_ids\"] = decoder_input_ids\n",
    "                            generate_kwargs.pop(\"decoder_start_token_id\", None)\n",
    "                            generated_ids = model.generate(**generate_kwargs)\n",
    "                        else:\n",
    "                            raise\n",
    "                \n",
    "                # Increment generation count\n",
    "                GENERATION_COUNT += 1\n",
    "                \n",
    "                # Decode - move to CPU for decoding\n",
    "                generated_ids_cpu = generated_ids.cpu()\n",
    "                chunk_text = tokenizer.decode(generated_ids_cpu[0], skip_special_tokens=True).strip()\n",
    "                \n",
    "                if chunk_text:\n",
    "                    full_transcription += chunk_text + \" \"\n",
    "                    successful_chunks += 1\n",
    "            \n",
    "            # Clear tensors\n",
    "            del inputs, input_features, generated_ids\n",
    "            if 'generated_ids_cpu' in locals():\n",
    "                del generated_ids_cpu\n",
    "            if 'decoder_input_ids' in locals():\n",
    "                del decoder_input_ids\n",
    "            \n",
    "            # Update display\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Processed {i+1}/{len(chunks)} chunks ({successful_chunks} successful)\")\n",
    "            print(f\"Generation count: {GENERATION_COUNT}\")\n",
    "            print(\"\\nCurrent Transcription:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(full_transcription.strip())\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Memory cleanup with XPU-specific handling\n",
    "            if (i + 1) % 3 == 0:\n",
    "                if device == 'xpu':\n",
    "                    torch.xpu.empty_cache()\n",
    "                    torch.xpu.synchronize()  # Ensure all operations are complete\n",
    "                gc.collect()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in chunk {i+1}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Try to continue with next chunk\n",
    "            if device == 'xpu':\n",
    "                torch.xpu.empty_cache()\n",
    "                torch.xpu.synchronize()\n",
    "            gc.collect()\n",
    "            continue\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n Completed in {inference_time:.2f} seconds\")\n",
    "    print(f\"   Successful chunks: {successful_chunks}/{len(chunks)}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return full_transcription.strip(), inference_time\n",
    "\n",
    "# Run transcription\n",
    "if 'model_components' in globals() and 'audio' in globals():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting streaming transcription...\")\n",
    "    print(\"=\"*50)\n",
    "    transcription, time_taken = transcribe_audio_streaming(model_components, audio)\n",
    "    print(\"\\nFinal Transcription:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(transcription if transcription else \"No transcription generated\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Total time: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. System Status and Memory Management\n",
    "\n",
    "### Code Walkthrough:\n",
    "\n",
    "• **Memory Monitoring**: Tracks XPU memory allocation and reservation in real-time\n",
    "\n",
    "• **Model Cache Management**: Provides utilities to clear cached models and free GPU memory\n",
    "\n",
    "• **System Status Display**: Shows current device, cached models, and memory usage\n",
    "\n",
    "• **Generation Tracking**: Monitors the number of inference operations for debugging memory leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_memory_usage():\n",
    "    \"\"\"\n",
    "    Monitor GPU memory usage and system status.\n",
    "    \"\"\"\n",
    "    if device == 'xpu' and hasattr(torch.xpu, 'memory_allocated'):\n",
    "        allocated = torch.xpu.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.xpu.memory_reserved() / 1024**3   # GB\n",
    "        print(f\"XPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "        print(f\"Generation count: {GENERATION_COUNT}\")\n",
    "    else:\n",
    "        print(\"Memory monitoring not available for this device\")\n",
    "\n",
    "def clear_all_models():\n",
    "    \"\"\"\n",
    "    Clear all cached models and free memory.\n",
    "    \"\"\"\n",
    "    global MODEL_CACHE, GENERATION_COUNT\n",
    "    \n",
    "    print(\"Clearing all cached models...\")\n",
    "    \n",
    "    for model_name in list(MODEL_CACHE.keys()):\n",
    "        try:\n",
    "            if 'model' in MODEL_CACHE[model_name]:\n",
    "                MODEL_CACHE[model_name]['model'].cpu()\n",
    "                del MODEL_CACHE[model_name]['model']\n",
    "            del MODEL_CACHE[model_name]\n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing {model_name}: {e}\")\n",
    "    \n",
    "    MODEL_CACHE.clear()\n",
    "    GENERATION_COUNT = 0\n",
    "    clear_gpu_memory()\n",
    "    print(\"All models cleared from cache\")\n",
    "\n",
    "# Check current status\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Current Status:\")\n",
    "print(\"=\"*50)\n",
    "monitor_memory_usage()\n",
    "print(f\"Cached models: {list(MODEL_CACHE.keys())}\")\n",
    "print(f\"Audio loaded: {'audio' in globals()}\")\n",
    "print(f\"Model loaded: {'model_components' in globals()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Audio Generation Tool\n",
    "\n",
    "### Code Walkthrough:\n",
    "\n",
    "• **Text-to-Speech (TTS) Features**:\n",
    "  - Uses Google TTS (gTTS) for converting text to speech\n",
    "  - Supports 5 languages: English, Spanish, French, German, Chinese\n",
    "  - Includes example prompts for quick testing\n",
    "  - Auto-detects language from selected examples\n",
    "\n",
    "• **Microphone Recording Features**:\n",
    "  - Uses sounddevice library for real-time audio capture\n",
    "  - Configurable recording duration (1-30 seconds)\n",
    "  - Device selection for multi-microphone setups\n",
    "  - Real-time progress indicator during recording\n",
    "\n",
    "• **Audio Processing**:\n",
    "  - Normalizes audio levels to prevent clipping\n",
    "  - Saves in standard WAV format (16kHz, 16-bit)\n",
    "  - Displays interactive audio player for immediate playback\n",
    "\n",
    "• **User Interface**:\n",
    "  - Tab-based interface for different audio sources\n",
    "  - Interactive widgets for parameter control\n",
    "  - Real-time status updates and error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_generation_tool():\n",
    "    \"\"\"\n",
    "    Interactive tool for generating audio through TTS or microphone recording.\n",
    "    \n",
    "    Provides two methods:\n",
    "    1. Text-to-Speech using gTTS library\n",
    "    2. Microphone recording using sounddevice\n",
    "    \"\"\"\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, Audio as IPythonAudio, clear_output\n",
    "    import numpy as np\n",
    "    import scipy.io.wavfile as wavfile\n",
    "    import os\n",
    "    import time\n",
    "    \n",
    "    # Try imports\n",
    "    try:\n",
    "        from gtts import gTTS\n",
    "        gtts_available = True\n",
    "    except:\n",
    "        gtts_available = False\n",
    "    \n",
    "    try:\n",
    "        import sounddevice as sd\n",
    "        sd_available = True\n",
    "    except:\n",
    "        sd_available = False\n",
    "    \n",
    "    if not gtts_available and not sd_available:\n",
    "        print(\"❌ Please install required packages:\")\n",
    "        print(\"   pip install gtts sounddevice\")\n",
    "        return\n",
    "    \n",
    "    # Create UI\n",
    "    method_tabs = widgets.Tab()\n",
    "    tabs_list = []\n",
    "    \n",
    "    # Text-to-Speech Tab\n",
    "    if gtts_available:\n",
    "        tts_container = widgets.VBox()\n",
    "        \n",
    "        # Example prompts dropdown\n",
    "        example_prompts = widgets.Dropdown(\n",
    "            options=[\n",
    "                ('Custom Text', ''),\n",
    "                ('-- English Examples --', ''),\n",
    "                ('Technical Documentation', 'The machine learning model uses a transformer architecture with attention mechanisms.'),\n",
    "                ('News Report', 'Breaking news: Scientists have discovered a new method for carbon capture.'),\n",
    "                ('-- Multilingual Examples --', ''),\n",
    "                ('Spanish', '¡Hola! Me llamo Ana y soy tu asistente virtual.'),\n",
    "                ('French', 'Bonjour, je suis ravi de vous rencontrer.'),\n",
    "                ('German', 'Bitte öffnen Sie die Datei und klicken Sie auf Speichern.'),\n",
    "                ('Chinese', '欢迎使用语音识别系统。'),\n",
    "            ],\n",
    "            value='',\n",
    "            description='Examples:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='600px')\n",
    "        )\n",
    "        \n",
    "        text_input = widgets.Textarea(\n",
    "            value='Hello, this is a test of the text to speech system.',\n",
    "            placeholder='Enter text to convert to speech...',\n",
    "            description='Text:',\n",
    "            layout=widgets.Layout(width='100%', height='100px')\n",
    "        )\n",
    "        \n",
    "        # Language dropdown - 5 main languages\n",
    "        language_dropdown = widgets.Dropdown(\n",
    "            options=[\n",
    "                ('English', 'en'),\n",
    "                ('Spanish', 'es'),\n",
    "                ('French', 'fr'),\n",
    "                ('German', 'de'),\n",
    "                ('Chinese (Simplified)', 'zh-cn'),\n",
    "            ],\n",
    "            value='en',\n",
    "            description='Language:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        filename_tts = widgets.Text(\n",
    "            value='generated_speech.mp3',\n",
    "            placeholder='filename.mp3',\n",
    "            description='Filename:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        generate_button = widgets.Button(\n",
    "            description='Generate Speech',\n",
    "            button_style='primary',\n",
    "            icon='volume-up'\n",
    "        )\n",
    "        \n",
    "        tts_output = widgets.Output()\n",
    "        \n",
    "        tts_container.children = [\n",
    "            widgets.HTML(\"<h4>Text-to-Speech Generator</h4>\"),\n",
    "            example_prompts,\n",
    "            text_input,\n",
    "            language_dropdown,\n",
    "            filename_tts,\n",
    "            generate_button,\n",
    "            tts_output\n",
    "        ]\n",
    "        \n",
    "        tabs_list.append(('Text-to-Speech', tts_container))\n",
    "    \n",
    "    # Microphone Recording Tab\n",
    "    if sd_available:\n",
    "        mic_container = widgets.VBox()\n",
    "        \n",
    "        # Device selection\n",
    "        device_dropdown = widgets.Dropdown(\n",
    "            description='Input Device:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # Get input devices\n",
    "        try:\n",
    "            devices = sd.query_devices()\n",
    "            input_devices = []\n",
    "            \n",
    "            for idx, device in enumerate(devices):\n",
    "                if device['max_input_channels'] > 0:\n",
    "                    device_name = f\"{idx}: {device['name']} ({device['max_input_channels']} ch)\"\n",
    "                    input_devices.append((device_name, idx))\n",
    "            \n",
    "            if not input_devices:\n",
    "                input_devices.append((\"Default Device\", None))\n",
    "            \n",
    "            device_dropdown.options = input_devices\n",
    "            if input_devices:\n",
    "                device_dropdown.value = input_devices[0][1]\n",
    "                \n",
    "        except Exception as e:\n",
    "            input_devices = [(\"Default Device\", None)]\n",
    "            device_dropdown.options = input_devices\n",
    "        \n",
    "        duration_slider = widgets.FloatSlider(\n",
    "            value=5.0,\n",
    "            min=1.0,\n",
    "            max=30.0,\n",
    "            step=0.5,\n",
    "            description='Duration (s):',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        filename_mic = widgets.Text(\n",
    "            value='recorded_audio.wav',\n",
    "            placeholder='filename.wav',\n",
    "            description='Filename:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        record_button = widgets.Button(\n",
    "            description='Start Recording',\n",
    "            button_style='danger',\n",
    "            icon='microphone'\n",
    "        )\n",
    "        \n",
    "        mic_output = widgets.Output()\n",
    "        \n",
    "        mic_container.children = [\n",
    "            widgets.HTML(\"<h4>Microphone Recorder</h4>\"),\n",
    "            device_dropdown,\n",
    "            duration_slider,\n",
    "            filename_mic,\n",
    "            record_button,\n",
    "            mic_output\n",
    "        ]\n",
    "        \n",
    "        tabs_list.append(('Microphone', mic_container))\n",
    "    \n",
    "    # Set up tabs\n",
    "    method_tabs.children = [tab[1] for tab in tabs_list]\n",
    "    for i, (title, _) in enumerate(tabs_list):\n",
    "        method_tabs.set_title(i, title)\n",
    "    \n",
    "    # Display\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Audio Generation Tool</h3>\"),\n",
    "        method_tabs\n",
    "    ]))\n",
    "    \n",
    "    # Event handlers\n",
    "    if gtts_available:\n",
    "        def update_text_from_example(change):\n",
    "            \"\"\"Update text input when example is selected\"\"\"\n",
    "            if change['new'] and not change['new'].startswith('--'):\n",
    "                text_input.value = change['new']\n",
    "                # Auto-select appropriate language\n",
    "                if 'Spanish' in example_prompts.label:\n",
    "                    language_dropdown.value = 'es'\n",
    "                elif 'French' in example_prompts.label:\n",
    "                    language_dropdown.value = 'fr'\n",
    "                elif 'German' in example_prompts.label:\n",
    "                    language_dropdown.value = 'de'\n",
    "                elif 'Chinese' in example_prompts.label:\n",
    "                    language_dropdown.value = 'zh-cn'\n",
    "                else:\n",
    "                    language_dropdown.value = 'en'\n",
    "        \n",
    "        example_prompts.observe(update_text_from_example, names='value')\n",
    "        \n",
    "        def on_generate_click(b):\n",
    "            \"\"\"Handle text-to-speech generation\"\"\"\n",
    "            with tts_output:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                text = text_input.value.strip()\n",
    "                if not text:\n",
    "                    print(\"❌ Please enter some text\")\n",
    "                    return\n",
    "                \n",
    "                try:\n",
    "                    # Get language name for display\n",
    "                    lang_name = next((name for name, code in language_dropdown.options if code == language_dropdown.value), language_dropdown.value)\n",
    "                    print(f\"🔊 Generating {lang_name} speech...\")\n",
    "                    \n",
    "                    # Create TTS object\n",
    "                    tts = gTTS(text=text, lang=language_dropdown.value, slow=False, tld='com')\n",
    "                    \n",
    "                    # Save to file\n",
    "                    filename = filename_tts.value\n",
    "                    if not filename.endswith('.mp3'):\n",
    "                        filename += '.mp3'\n",
    "                    \n",
    "                    filepath = os.path.join(os.getcwd(), filename)\n",
    "                    tts.save(filepath)\n",
    "                    \n",
    "                    print(f\"✅ Audio saved: {filename}\")\n",
    "                    print(f\"   Size: {os.path.getsize(filepath) / 1024:.1f} KB\")\n",
    "                    print(f\"   Language: {lang_name}\")\n",
    "                    \n",
    "                    # Display audio player\n",
    "                    display(IPythonAudio(filepath))\n",
    "                    \n",
    "                    print(f\"\\n📝 Generated text ({len(text)} chars):\")\n",
    "                    print(f'\"{text[:200]}{\"...\" if len(text) > 200 else \"\"}\"')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error: {str(e)}\")\n",
    "                    print(\"Note: gTTS requires internet connection\")\n",
    "        \n",
    "        generate_button.on_click(on_generate_click)\n",
    "    \n",
    "    if sd_available:\n",
    "        recording_state = {'is_recording': False}\n",
    "        \n",
    "        def on_record_click(b):\n",
    "            \"\"\"Handle microphone recording\"\"\"\n",
    "            if recording_state['is_recording']:\n",
    "                return\n",
    "            \n",
    "            with mic_output:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                duration = duration_slider.value\n",
    "                sample_rate = 16000  # Fixed for speech\n",
    "                device_id = device_dropdown.value\n",
    "                \n",
    "                try:\n",
    "                    recording_state['is_recording'] = True\n",
    "                    record_button.description = 'Recording...'\n",
    "                    record_button.button_style = 'warning'\n",
    "                    record_button.disabled = True\n",
    "                    \n",
    "                    print(f\"🎤 Recording for {duration:.1f} seconds...\")\n",
    "                    print(\"\\n⏱️  \", end='', flush=True)\n",
    "                    \n",
    "                    # Record audio\n",
    "                    if device_id is not None:\n",
    "                        audio_data = sd.rec(int(duration * sample_rate), \n",
    "                                          samplerate=sample_rate, \n",
    "                                          channels=1, \n",
    "                                          device=device_id,\n",
    "                                          dtype='float32')\n",
    "                    else:\n",
    "                        audio_data = sd.rec(int(duration * sample_rate), \n",
    "                                          samplerate=sample_rate, \n",
    "                                          channels=1,\n",
    "                                          dtype='float32')\n",
    "                    \n",
    "                    # Show progress\n",
    "                    start_time = time.time()\n",
    "                    while time.time() - start_time < duration:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        progress = int((elapsed / duration) * 20)\n",
    "                        print(f\"\\r⏱️  [{'█' * progress}{'░' * (20 - progress)}] {elapsed:.1f}s\", \n",
    "                              end='', flush=True)\n",
    "                        time.sleep(0.1)\n",
    "                    \n",
    "                    sd.wait()\n",
    "                    print(f\"\\r⏱️  [{'█' * 20}] {duration:.1f}s\")\n",
    "                    \n",
    "                    # Process and save\n",
    "                    audio_data = audio_data.flatten()\n",
    "                    max_val = np.max(np.abs(audio_data))\n",
    "                    if max_val > 0:\n",
    "                        audio_data = audio_data / max_val * 0.9\n",
    "                    \n",
    "                    filename = filename_mic.value\n",
    "                    if not filename.endswith('.wav'):\n",
    "                        filename += '.wav'\n",
    "                    \n",
    "                    filepath = os.path.join(os.getcwd(), filename)\n",
    "                    audio_int16 = (audio_data * 32767).astype(np.int16)\n",
    "                    wavfile.write(filepath, sample_rate, audio_int16)\n",
    "                    \n",
    "                    print(f\"\\n✅ Audio saved: {filename}\")\n",
    "                    print(f\"   Duration: {duration}s\")\n",
    "                    print(f\"   Sample rate: 16kHz\")\n",
    "                    \n",
    "                    # Display audio player\n",
    "                    display(IPythonAudio(audio_data, rate=sample_rate))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error: {str(e)}\")\n",
    "                \n",
    "                finally:\n",
    "                    recording_state['is_recording'] = False\n",
    "                    record_button.description = 'Start Recording'\n",
    "                    record_button.button_style = 'danger'\n",
    "                    record_button.disabled = False\n",
    "        \n",
    "        record_button.on_click(on_record_click)\n",
    "    \n",
    "    # Initial message\n",
    "    print(\"Ready to generate audio files!\")\n",
    "    if gtts_available:\n",
    "        print(\"• Text-to-Speech: Select examples or enter custom text\")\n",
    "    if sd_available:\n",
    "        print(\"• Microphone: Record your voice\")\n",
    "\n",
    "# Run the tool\n",
    "audio_generation_tool()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Streaming Transcription Application\n",
    "\n",
    "### Code Walkthrough:\n",
    "\n",
    "• **Model Selection Interface**:\n",
    "  - Dropdown with English-only and multilingual Whisper variants\n",
    "  - Automatic detection of model capabilities\n",
    "  - Support for both Distil-Whisper (faster) and standard Whisper models\n",
    "\n",
    "• **Task Configuration**:\n",
    "  - **Transcribe**: Convert speech to text in original language\n",
    "  - **Translate**: Convert foreign speech to English text\n",
    "  - Language selection for multilingual models (5 main languages)\n",
    "\n",
    "• **Audio Input Options**:\n",
    "  - LibriSpeech dataset samples (streaming, no download required)\n",
    "  - File upload support (MP3, WAV, M4A, FLAC, OGG)\n",
    "  - Uses librosa for robust audio loading\n",
    "\n",
    "• **Performance Features**:\n",
    "  - Real-time progress tracking with chunk counter\n",
    "  - Performance metrics (inference time, real-time factor)\n",
    "  - Comparison with previous runs\n",
    "  - Model caching to avoid reloading\n",
    "\n",
    "• **XPU Optimizations**:\n",
    "  - Precision selection (FP16/FP32)\n",
    "  - Automatic mixed precision for XPU\n",
    "  - Memory management with periodic cleanup\n",
    "\n",
    "• **User Experience**:\n",
    "  - Interactive progress bar during processing\n",
    "  - Live transcription updates\n",
    "  - Audio preview before processing\n",
    "  - Clear error messages and recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_transcription_app():\n",
    "    \"\"\"\n",
    "    Interactive Whisper transcription application with streaming capabilities.\n",
    "    \n",
    "    This application provides a user-friendly interface for:\n",
    "    - Loading various Whisper models (English-only and multilingual)\n",
    "    - Transcribing audio from LibriSpeech datasets or uploaded files\n",
    "    - Translating audio to English (multilingual models only)\n",
    "    - Real-time progress tracking during transcription\n",
    "    - GPU memory management for sustained usage\n",
    "    \n",
    "    The app supports Intel XPU acceleration when available and automatically\n",
    "    handles model caching to improve performance.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from datasets import load_dataset\n",
    "    from IPython.display import display, clear_output\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import Audio as IPythonAudio\n",
    "    import os\n",
    "    import tempfile\n",
    "    import librosa\n",
    "    import gc\n",
    "    \n",
    "    global MODEL_CACHE, device, dtype\n",
    "    \n",
    "    # Track inference history\n",
    "    if not hasattr(streaming_transcription_app, 'inference_history'):\n",
    "        streaming_transcription_app.inference_history = []\n",
    "    \n",
    "    if hasattr(streaming_transcription_app, \"is_running\") and streaming_transcription_app.is_running:\n",
    "        print(\"Application is already running. Please wait or restart the kernel.\")\n",
    "        return\n",
    "    \n",
    "    streaming_transcription_app.is_running = True\n",
    "    \n",
    "    try:\n",
    "        # Model dropdown\n",
    "        model_dropdown = widgets.Dropdown(\n",
    "            options=[\n",
    "                (' Distil Whisper Small EN', 'distil-whisper/distil-small.en'),\n",
    "                (' Whisper Base EN', 'openai/whisper-base.en'),\n",
    "                ('Whisper Tiny EN', 'openai/whisper-tiny.en'),\n",
    "                ('Distil Whisper Medium EN', 'distil-whisper/distil-medium.en'),\n",
    "                ('--- Multilingual Models ---', None),\n",
    "                (' Whisper Base Multi', 'openai/whisper-base'),\n",
    "                (' Whisper Small Multi', 'openai/whisper-small'),\n",
    "                (' Whisper Medium Multi', 'openai/whisper-medium'),\n",
    "                (' Distil Whisper Large v2 Multi', 'distil-whisper/distil-large-v2'),\n",
    "                (' Distil Whisper Large v3 Multi', 'distil-whisper/distil-large-v3'),\n",
    "            ],\n",
    "            value='distil-whisper/distil-small.en',\n",
    "            description='Model:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Task selector\n",
    "        task_radio = widgets.RadioButtons(\n",
    "            options=['Transcribe', 'Translate to English'],\n",
    "            value='Transcribe',\n",
    "            description='Task:',\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        # Language selector - 5 main languages only\n",
    "        target_lang_dropdown = widgets.Dropdown(\n",
    "            options=[\n",
    "                ('Auto-detect', 'auto'),\n",
    "                ('English', 'en'),\n",
    "                ('Spanish', 'es'),\n",
    "                ('French', 'fr'),\n",
    "                ('German', 'de'),\n",
    "                ('Chinese', 'zh'),\n",
    "            ],\n",
    "            value='auto',\n",
    "            description='Language:',\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        # Precision selector\n",
    "        precision_radio = widgets.RadioButtons(\n",
    "            options=['FP16 (Faster)', 'FP32 (More Stable)'],\n",
    "            value='FP16 (Faster)' if device == 'xpu' else 'FP32 (More Stable)',\n",
    "            description='Precision:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        source_radio = widgets.RadioButtons(\n",
    "            options=['Sample Dataset', 'Upload File'],\n",
    "            value='Sample Dataset',\n",
    "            description='Audio Source:'\n",
    "        )\n",
    "        \n",
    "        # Simplified LibriSpeech datasets\n",
    "        dataset_dropdown = widgets.Dropdown(\n",
    "            options=[\n",
    "                ('LibriSpeech Long - Clean', 'librispeech_long'),                \n",
    "            ],\n",
    "            value='librispeech_long',\n",
    "            description='Dataset:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        file_upload = widgets.FileUpload(\n",
    "            accept='.mp3, .wav, .m4a, .flac, .ogg',\n",
    "            multiple=False,\n",
    "            description='Upload Audio:'\n",
    "        )\n",
    "        \n",
    "        load_button = widgets.Button(\n",
    "            description='Load Model & Audio',\n",
    "            button_style='primary'\n",
    "        )\n",
    "        \n",
    "        clear_memory_button = widgets.Button(\n",
    "            description='Clear GPU Memory',\n",
    "            button_style='warning'\n",
    "        )\n",
    "        \n",
    "        model_info = widgets.HTML(value=\"\")\n",
    "        \n",
    "        setup_output = widgets.Output()\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(value=\"<h3>Whisper Transcription & Translation</h3>\"),\n",
    "            model_dropdown,\n",
    "            widgets.HBox([task_radio, target_lang_dropdown]),\n",
    "            precision_radio,\n",
    "            model_info,\n",
    "            source_radio,\n",
    "            dataset_dropdown,\n",
    "            file_upload,\n",
    "            widgets.HBox([load_button, clear_memory_button]),\n",
    "            setup_output\n",
    "        ]))\n",
    "        \n",
    "        app_container = widgets.Output()\n",
    "        display(app_container)\n",
    "        \n",
    "        # State variables\n",
    "        current_model_name = None\n",
    "        current_model_components = None\n",
    "        audio_data = None\n",
    "        \n",
    "        def update_model_info(change):\n",
    "            \"\"\"\n",
    "            Update the model information display based on selected model.\n",
    "            \n",
    "            Args:\n",
    "                change: Widget change event (optional)\n",
    "            \"\"\"\n",
    "            if model_dropdown.value is None:\n",
    "                return\n",
    "                \n",
    "            model_name = model_dropdown.value\n",
    "            info = \"<b>Model:</b> \"\n",
    "            is_multilingual = not any(x in model_name for x in ['.en', 'EN'])\n",
    "            \n",
    "            task_radio.disabled = not is_multilingual\n",
    "            target_lang_dropdown.disabled = not is_multilingual\n",
    "            \n",
    "            if not is_multilingual:\n",
    "                task_radio.value = 'Transcribe'\n",
    "                info += \"English-only\"\n",
    "            else:\n",
    "                info += \"Multilingual (96+ languages)\"\n",
    "            \n",
    "            model_info.value = info\n",
    "        \n",
    "        model_dropdown.observe(update_model_info, names='value')\n",
    "        update_model_info(None)\n",
    "        \n",
    "        def on_source_change(change):\n",
    "            \"\"\"\n",
    "            Handle audio source selection changes.\n",
    "            \n",
    "            Args:\n",
    "                change: Widget change event\n",
    "            \"\"\"\n",
    "            if change['new'] == 'Upload File':\n",
    "                file_upload.layout.display = 'block'\n",
    "                dataset_dropdown.layout.display = 'none'\n",
    "            else:\n",
    "                file_upload.layout.display = 'none'\n",
    "                dataset_dropdown.layout.display = 'block'\n",
    "        \n",
    "        source_radio.observe(on_source_change, names='value')\n",
    "        file_upload.layout.display = 'none'\n",
    "        \n",
    "        def clear_memory_handler(b):\n",
    "            \"\"\"\n",
    "            Handle clear memory button click.\n",
    "            \n",
    "            Args:\n",
    "                b: Button widget\n",
    "            \"\"\"\n",
    "            with setup_output:\n",
    "                clear_output()\n",
    "                clear_gpu_memory()\n",
    "                print(\"GPU memory cleared\")\n",
    "        \n",
    "        clear_memory_button.on_click(clear_memory_handler)\n",
    "        \n",
    "        def load_audio_from_upload(uploaded_file):\n",
    "            \"\"\"\n",
    "            Load audio from uploaded file.\n",
    "            \n",
    "            Args:\n",
    "                uploaded_file: FileUpload widget data\n",
    "                \n",
    "            Returns:\n",
    "                dict: Audio dictionary with 'array' and 'sampling_rate' keys, or None if failed\n",
    "            \"\"\"\n",
    "            print(f\"Loading: {uploaded_file.name}\")\n",
    "            \n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{uploaded_file.name.split('.')[-1]}\") as tmp_file:\n",
    "                tmp_file.write(uploaded_file.content)\n",
    "                tmp_path = tmp_file.name\n",
    "            \n",
    "            try:\n",
    "                y, sr = librosa.load(tmp_path, sr=16000, mono=True)\n",
    "                audio = {\n",
    "                    \"array\": y.astype(np.float32),\n",
    "                    \"sampling_rate\": sr\n",
    "                }\n",
    "                os.unlink(tmp_path)\n",
    "                print(f\"Loaded: {len(y)/sr:.2f} seconds\")\n",
    "                return audio\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                try:\n",
    "                    os.unlink(tmp_path)\n",
    "                except:\n",
    "                    pass\n",
    "                return None\n",
    "        \n",
    "        def load_sample_audio_from_dataset(dataset_name):\n",
    "            \"\"\"\n",
    "            Load audio sample from LibriSpeech dataset.\n",
    "            \n",
    "            Args:\n",
    "                dataset_name (str): Dataset identifier\n",
    "                \n",
    "            Returns:\n",
    "                dict: Audio dictionary with 'array' and 'sampling_rate' keys\n",
    "            \"\"\"\n",
    "            print(f\"Loading {dataset_name}...\")\n",
    "            \n",
    "            try:\n",
    "                if dataset_name == 'librispeech_asr_test':\n",
    "                    # Load short test samples\n",
    "                    dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\", streaming=True)\n",
    "                else:\n",
    "                    # Load standard long samples\n",
    "                    dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\", streaming=True)\n",
    "                \n",
    "                sample = next(iter(dataset))\n",
    "                audio = sample['audio']\n",
    "                print(f\"Loaded: {len(audio['array'])/audio['sampling_rate']:.2f} seconds\")\n",
    "                return audio\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)[:100]}...\")\n",
    "                print(\"Using fallback dataset...\")\n",
    "                dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\", streaming=True)\n",
    "                sample = next(iter(dataset))\n",
    "                return sample[\"audio\"]\n",
    "        \n",
    "        def load_whisper_model_with_lang(model_name, language=\"auto\", task=\"transcribe\"):\n",
    "            \"\"\"\n",
    "            Load Whisper model with language and task configuration.\n",
    "            \n",
    "            Args:\n",
    "                model_name (str): HuggingFace model identifier\n",
    "                language (str): Target language code or \"auto\"\n",
    "                task (str): \"transcribe\" or \"translate\"\n",
    "                \n",
    "            Returns:\n",
    "                dict: Model components dictionary\n",
    "            \"\"\"\n",
    "            global MODEL_CACHE, dtype, device\n",
    "            \n",
    "            cache_key = f\"{model_name}_{language}_{task}\"\n",
    "            if cache_key in MODEL_CACHE:\n",
    "                print(f\"Using cached model: {model_name}\")\n",
    "                return MODEL_CACHE[cache_key]\n",
    "            \n",
    "            if len(MODEL_CACHE) > 2:\n",
    "                print(\"Clearing model cache...\")\n",
    "                MODEL_CACHE.clear()\n",
    "                clear_gpu_memory()\n",
    "            \n",
    "            print(f\"Loading: {model_name}\")\n",
    "            \n",
    "            is_english_only = any(x in model_name.lower() for x in ['.en', 'english-only'])\n",
    "            \n",
    "            tokenizer = WhisperTokenizer.from_pretrained(model_name)\n",
    "            processor = AutoProcessor.from_pretrained(model_name)\n",
    "            \n",
    "            model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=dtype,\n",
    "                low_cpu_mem_usage=True,\n",
    "                use_safetensors=True\n",
    "            )\n",
    "            \n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            model.config.forced_decoder_ids = None\n",
    "            model.config.suppress_tokens = []\n",
    "            \n",
    "            if model.config.decoder_start_token_id is None:\n",
    "                if is_english_only:\n",
    "                    model.config.decoder_start_token_id = tokenizer.pad_token_id or 50257\n",
    "                    model.config.forced_decoder_ids = [[1, 50259], [2, 50359], [3, 50363]]\n",
    "                else:\n",
    "                    model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "            \n",
    "            feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            MODEL_CACHE[cache_key] = {\n",
    "                'model': model,\n",
    "                'feature_extractor': feature_extractor,\n",
    "                'tokenizer': tokenizer,\n",
    "                'processor': processor,\n",
    "                'dtype': dtype,\n",
    "                'is_english_only': is_english_only,\n",
    "                'language': language,\n",
    "                'task': task\n",
    "            }\n",
    "            \n",
    "            print(f\" Model loaded\")\n",
    "            return MODEL_CACHE[cache_key]\n",
    "        \n",
    "        def on_load_button_click(b):\n",
    "            \"\"\"\n",
    "            Handle load button click to initialize model and audio.\n",
    "            \n",
    "            Args:\n",
    "                b: Button widget\n",
    "            \"\"\"\n",
    "            nonlocal current_model_name, current_model_components, audio_data\n",
    "            \n",
    "            if hasattr(on_load_button_click, \"is_running\") and on_load_button_click.is_running:\n",
    "                return\n",
    "            on_load_button_click.is_running = True\n",
    "            \n",
    "            setup_output.clear_output()\n",
    "            app_container.clear_output()\n",
    "            \n",
    "            with setup_output:\n",
    "                try:\n",
    "                    load_button.disabled = True\n",
    "                    \n",
    "                    if model_dropdown.value is None:\n",
    "                        print(\"Please select valid model\")\n",
    "                        load_button.disabled = False\n",
    "                        on_load_button_click.is_running = False\n",
    "                        return\n",
    "                    \n",
    "                    global dtype\n",
    "                    dtype = torch.float16 if \"FP16\" in precision_radio.value else torch.float32\n",
    "                    print(f\"Precision: {dtype}\")\n",
    "                    \n",
    "                    model_name = model_dropdown.value\n",
    "                    task = \"translate\" if \"Translate\" in task_radio.value else \"transcribe\"\n",
    "                    language = target_lang_dropdown.value if target_lang_dropdown.value != \"auto\" else None\n",
    "                    \n",
    "                    current_model_components = load_whisper_model_with_lang(\n",
    "                        model_name, \n",
    "                        language=language or \"auto\", \n",
    "                        task=task\n",
    "                    )\n",
    "                    current_model_name = model_name\n",
    "                    \n",
    "                    print(\"\\nLoading audio...\")\n",
    "                    if source_radio.value == 'Upload File':\n",
    "                        if not file_upload.value:\n",
    "                            print(\"Please upload an audio file.\")\n",
    "                            load_button.disabled = False\n",
    "                            on_load_button_click.is_running = False\n",
    "                            return\n",
    "                        audio_data = load_audio_from_upload(file_upload.value[0])\n",
    "                    else:\n",
    "                        audio_data = load_sample_audio_from_dataset(dataset_dropdown.value)\n",
    "                    \n",
    "                    if audio_data is None:\n",
    "                        print(\"Failed to load audio.\")\n",
    "                        load_button.disabled = False\n",
    "                        on_load_button_click.is_running = False\n",
    "                        return\n",
    "                    \n",
    "                    print(\"\\n Ready!\")\n",
    "                    setup_transcription_interface(current_model_components, audio_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    load_button.disabled = False\n",
    "                finally:\n",
    "                    on_load_button_click.is_running = False\n",
    "        \n",
    "        def transcribe_with_translation(model_components, audio, chunk_length_s=10):\n",
    "            \"\"\"\n",
    "            Perform streaming transcription or translation with chunk processing and XPU optimizations.\n",
    "            \n",
    "            Args:\n",
    "                model_components (dict): Model components dictionary\n",
    "                audio (dict): Audio dictionary with 'array' and 'sampling_rate'\n",
    "                chunk_length_s (int): Chunk length in seconds\n",
    "                \n",
    "            Returns:\n",
    "                tuple: (transcription_text, inference_time, performance_metrics)\n",
    "            \"\"\"\n",
    "            model = model_components['model']\n",
    "            processor = model_components['processor']\n",
    "            tokenizer = model_components['tokenizer']\n",
    "            model_dtype = model_components.get('dtype', torch.float32)\n",
    "            is_english_only = model_components.get('is_english_only', False)\n",
    "            task = model_components.get('task', 'transcribe')\n",
    "            language = model_components.get('language', 'auto')\n",
    "            \n",
    "            audio_array = audio[\"array\"]\n",
    "            sampling_rate = audio[\"sampling_rate\"]\n",
    "            chunk_size_samples = int(chunk_length_s * sampling_rate)\n",
    "            \n",
    "            chunks = []\n",
    "            for i in range(0, len(audio_array), chunk_size_samples):\n",
    "                chunk = audio_array[i:i+chunk_size_samples]\n",
    "                if len(chunk) >= sampling_rate * 0.5:\n",
    "                    chunks.append(chunk)\n",
    "            \n",
    "            print(f\"Processing {len(chunks)} chunks - Task: {task}\")\n",
    "            print(f\"Using {device} with {model_dtype}\")\n",
    "            \n",
    "            full_transcription = \"\"\n",
    "            start_time = time.time()\n",
    "            successful_chunks = 0\n",
    "            \n",
    "            # Setup autocast for XPU\n",
    "            autocast_dtype = model_dtype if device == 'xpu' else torch.float32\n",
    "            use_autocast = device == 'xpu' and model_dtype in [torch.float16, torch.bfloat16]\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                try:\n",
    "                    print(f\"Chunk {i+1}/{len(chunks)}...\")\n",
    "                    \n",
    "                    inputs = processor(chunk, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "                    input_features = inputs.input_features.to(device=device, dtype=model_dtype)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        with torch.autocast(device_type=device, dtype=autocast_dtype, enabled=use_autocast):\n",
    "                            chunk_duration = len(chunk) / sampling_rate\n",
    "                            max_tokens = min(100, int(chunk_duration * 20))\n",
    "                            \n",
    "                            generate_kwargs = {\n",
    "                                \"inputs\": input_features,\n",
    "                                \"max_new_tokens\": max_tokens,\n",
    "                                \"num_beams\": 1,\n",
    "                                \"do_sample\": False,\n",
    "                                \"use_cache\": False,\n",
    "                                \"return_dict_in_generate\": False,\n",
    "                            }\n",
    "                            \n",
    "                            if not is_english_only:\n",
    "                                if task == \"translate\":\n",
    "                                    generate_kwargs[\"language\"] = language if language != \"auto\" else None\n",
    "                                    generate_kwargs[\"task\"] = \"translate\"\n",
    "                                else:\n",
    "                                    generate_kwargs[\"language\"] = language if language != \"auto\" else None\n",
    "                                    generate_kwargs[\"task\"] = \"transcribe\"\n",
    "                            \n",
    "                            if model.config.decoder_start_token_id is not None:\n",
    "                                generate_kwargs[\"decoder_start_token_id\"] = model.config.decoder_start_token_id\n",
    "                            \n",
    "                            if hasattr(model.config, 'forced_decoder_ids') and model.config.forced_decoder_ids:\n",
    "                                generate_kwargs[\"forced_decoder_ids\"] = model.config.forced_decoder_ids\n",
    "                            \n",
    "                            try:\n",
    "                                generated_ids = model.generate(**generate_kwargs)\n",
    "                            except Exception as e:\n",
    "                                if \"language\" in str(e) or \"task\" in str(e):\n",
    "                                    generate_kwargs.pop(\"language\", None)\n",
    "                                    generate_kwargs.pop(\"task\", None)\n",
    "                                    generated_ids = model.generate(**generate_kwargs)\n",
    "                                else:\n",
    "                                    raise\n",
    "                            \n",
    "                            # Move to CPU for decoding\n",
    "                            generated_ids_cpu = generated_ids.cpu()\n",
    "                            chunk_text = tokenizer.decode(generated_ids_cpu[0], skip_special_tokens=True).strip()\n",
    "                            \n",
    "                            if chunk_text:\n",
    "                                full_transcription += chunk_text + \" \"\n",
    "                                successful_chunks += 1\n",
    "                    \n",
    "                    del inputs, input_features, generated_ids, generated_ids_cpu\n",
    "                    \n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"Progress: {i+1}/{len(chunks)} chunks\")\n",
    "                    print(\"\\nCurrent output:\")\n",
    "                    print(\"-\" * 80)\n",
    "                    print(full_transcription.strip())\n",
    "                    print(\"-\" * 80)\n",
    "                    \n",
    "                    if (i + 1) % 3 == 0:\n",
    "                        if device == 'xpu':\n",
    "                            torch.xpu.empty_cache()\n",
    "                            torch.xpu.synchronize()\n",
    "                        gc.collect()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in chunk {i+1}: {str(e)}\")\n",
    "                    if device == 'xpu':\n",
    "                        torch.xpu.empty_cache()\n",
    "                        torch.xpu.synchronize()\n",
    "                    continue\n",
    "            \n",
    "            end_time = time.time()\n",
    "            inference_time = end_time - start_time\n",
    "            audio_duration = len(audio_array) / sampling_rate\n",
    "            rtf = audio_duration / inference_time if inference_time > 0 else 0\n",
    "            \n",
    "            # Performance metrics\n",
    "            performance_metrics = {\n",
    "                'audio_duration': audio_duration,\n",
    "                'inference_time': inference_time,\n",
    "                'rtf': rtf,\n",
    "                'chunks_processed': successful_chunks,\n",
    "                'total_chunks': len(chunks),\n",
    "                'model': current_model_name,\n",
    "                'device': device,\n",
    "                'precision': str(model_dtype)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n Done in {inference_time:.2f}s ({successful_chunks}/{len(chunks)} chunks)\")\n",
    "            print(f\"   Audio duration: {audio_duration:.2f}s\")\n",
    "            print(f\"   Real-time factor: {rtf:.2f}x\")\n",
    "            \n",
    "            clear_gpu_memory()\n",
    "            return full_transcription.strip(), inference_time, performance_metrics\n",
    "        \n",
    "        def setup_transcription_interface(model_components, audio_data):\n",
    "            \"\"\"\n",
    "            Set up the transcription interface with controls and output areas.\n",
    "            \n",
    "            Args:\n",
    "                model_components (dict): Model components dictionary\n",
    "                audio_data (dict): Audio data dictionary\n",
    "            \"\"\"\n",
    "            with app_container:\n",
    "                clear_output()\n",
    "                \n",
    "                print(\"Audio preview:\")\n",
    "                display(IPythonAudio(audio_data[\"array\"], rate=audio_data[\"sampling_rate\"]))\n",
    "                \n",
    "                duration = len(audio_data[\"array\"]) / audio_data[\"sampling_rate\"]\n",
    "                print(f\"Duration: {duration:.2f}s | Model: {current_model_name}\")\n",
    "                \n",
    "                chunk_slider = widgets.IntSlider(\n",
    "                    value=8 if model_components['dtype'] == torch.float16 else 10,\n",
    "                    min=3, \n",
    "                    max=15,\n",
    "                    step=1,\n",
    "                    description='Chunk size (s):'\n",
    "                )\n",
    "                \n",
    "                start_button = widgets.Button(\n",
    "                    description='Start',\n",
    "                    button_style='success'\n",
    "                )\n",
    "                \n",
    "                progress_bar = widgets.IntProgress(\n",
    "                    value=0, min=0, max=100,\n",
    "                    description='Progress:'\n",
    "                )\n",
    "                \n",
    "                status_text = widgets.HTML(value=\"Ready\")\n",
    "                \n",
    "                # Performance comparison display\n",
    "                performance_display = widgets.HTML(value=\"\")\n",
    "                \n",
    "                output_label = \"Translation:\" if model_components.get('task') == 'translate' else \"Transcription:\"\n",
    "                transcription_area = widgets.Textarea(\n",
    "                    value='',\n",
    "                    placeholder=f'{output_label} will appear here...',\n",
    "                    description=output_label,\n",
    "                    layout=widgets.Layout(width='100%', height='200px')\n",
    "                )\n",
    "                \n",
    "                output = widgets.Output()\n",
    "                \n",
    "                display(widgets.VBox([\n",
    "                    chunk_slider,\n",
    "                    start_button,\n",
    "                    progress_bar,\n",
    "                    status_text,\n",
    "                    performance_display,\n",
    "                    transcription_area,\n",
    "                    output\n",
    "                ]))\n",
    "                \n",
    "                def on_start_click(b):\n",
    "                    \"\"\"\n",
    "                    Handle start button click to begin transcription.\n",
    "                    \n",
    "                    Args:\n",
    "                        b: Button widget\n",
    "                    \"\"\"\n",
    "                    if hasattr(on_start_click, \"is_running\") and on_start_click.is_running:\n",
    "                        return\n",
    "                    \n",
    "                    on_start_click.is_running = True\n",
    "                    \n",
    "                    try:\n",
    "                        transcription_area.value = \"\"\n",
    "                        progress_bar.value = 0\n",
    "                        start_button.disabled = True\n",
    "                        output.clear_output()\n",
    "                        \n",
    "                        chunk_length_s = chunk_slider.value\n",
    "                        \n",
    "                        with output:\n",
    "                            original_print = print\n",
    "                            \n",
    "                            def custom_print(*args, **kwargs):\n",
    "                                \"\"\"Custom print to update progress bar\"\"\"\n",
    "                                msg = ' '.join(str(arg) for arg in args)\n",
    "                                if \"Chunk\" in msg and \"/\" in msg:\n",
    "                                    try:\n",
    "                                        parts = msg.split()\n",
    "                                        for i, part in enumerate(parts):\n",
    "                                            if \"/\" in part:\n",
    "                                                current = int(parts[i-1])\n",
    "                                                total = int(part.split(\"/\")[1])\n",
    "                                                progress_bar.max = total\n",
    "                                                progress_bar.value = current\n",
    "                                                status_text.value = f\"<b>Processing {current}/{total}</b>\"\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                original_print(*args, **kwargs)\n",
    "                            \n",
    "                            import builtins\n",
    "                            builtins.print = custom_print\n",
    "                            \n",
    "                            try:\n",
    "                                result, time_taken, metrics = transcribe_with_translation(\n",
    "                                    model_components, \n",
    "                                    audio_data, \n",
    "                                    chunk_length_s=chunk_length_s\n",
    "                                )\n",
    "                                \n",
    "                                transcription_area.value = result\n",
    "                                \n",
    "                                # Update performance display\n",
    "                                streaming_transcription_app.inference_history.append(metrics)\n",
    "                                \n",
    "                                # Show current and previous performance\n",
    "                                perf_html = \"<b>Performance Metrics:</b><br>\"\n",
    "                                perf_html += f\"<b>Current Run:</b> {metrics['inference_time']:.2f}s | {metrics['rtf']:.2f}x realtime<br>\"\n",
    "                                \n",
    "                                if len(streaming_transcription_app.inference_history) > 1:\n",
    "                                    prev_metrics = streaming_transcription_app.inference_history[-2]\n",
    "                                    speed_diff = metrics['rtf'] - prev_metrics['rtf']\n",
    "                                    speed_color = 'green' if speed_diff > 0 else 'red'\n",
    "                                    perf_html += f\"<b>Previous Run:</b> {prev_metrics['inference_time']:.2f}s | {prev_metrics['rtf']:.2f}x realtime<br>\"\n",
    "                                    perf_html += f\"<b>Speed Change:</b> <span style='color: {speed_color}'>{speed_diff:+.2f}x</span>\"\n",
    "                                \n",
    "                                performance_display.value = perf_html\n",
    "                                \n",
    "                                status_text.value = f\"<b>Done! {time_taken:.1f}s ({metrics['rtf']:.1f}x realtime)</b>\"\n",
    "                                \n",
    "                            finally:\n",
    "                                builtins.print = original_print\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        with output:\n",
    "                            print(f\"Error: {e}\")\n",
    "                        status_text.value = \"<b style='color: red;'>Error occurred</b>\"\n",
    "                    finally:\n",
    "                        start_button.disabled = False\n",
    "                        on_start_click.is_running = False\n",
    "                        clear_gpu_memory()\n",
    "                \n",
    "                start_button.on_click(on_start_click)\n",
    "                load_button.disabled = False\n",
    "        \n",
    "        load_button.on_click(on_load_button_click)\n",
    "        \n",
    "        with setup_output:\n",
    "            print(\"Ready to transcribe!\")\n",
    "            print(\"\\nMultilingual models can:\")\n",
    "            print(\"- Transcribe audio in multiple languages\")\n",
    "            print(\"- Translate foreign audio to English\")\n",
    "            print(\"- Auto-detect language or specify manually\")\n",
    "        \n",
    "    finally:\n",
    "        streaming_transcription_app.is_running = False\n",
    "\n",
    "# Run the app\n",
    "streaming_transcription_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Cleanup\n",
    "\n",
    "### Code Walkthrough:\n",
    "\n",
    "• **Application Features Summary**: Lists all implemented capabilities and optimizations\n",
    "\n",
    "• **Memory Cleanup Utilities**: \n",
    "  - `clear_all_models()`: Removes all cached models from memory\n",
    "  - `check_memory_status()`: Displays current cache and device status\n",
    "\n",
    "• **Best Practices**: Demonstrates proper cleanup procedures for long-running applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and cleanup\n",
    "print(\"\"\"\n",
    "Application Features:\n",
    "1. Model caching - prevents reloading the same model\n",
    "2. Memory management - clears GPU memory periodically\n",
    "3. Error handling - prevents crashes during transcription\n",
    "4. Support for English-only and multilingual models\n",
    "5. Optimized chunk processing for stability\n",
    "6. Text-to-speech and microphone recording for audio generation\n",
    "7. Support for multiple languages and translation\n",
    "\n",
    "To free GPU memory, use the 'Clear GPU Memory' button in the app.\n",
    "\"\"\")\n",
    "\n",
    "# Optional: Clear all cached models\n",
    "def clear_all_models():\n",
    "    \"\"\"Clear all cached models and free memory\"\"\"\n",
    "    global MODEL_CACHE\n",
    "    MODEL_CACHE.clear()\n",
    "    clear_gpu_memory()\n",
    "    print(\"✅ All models cleared from cache\")\n",
    "    print(\"   GPU/XPU memory freed\")\n",
    "\n",
    "# Clear all models\n",
    "clear_all_models()\n",
    "\n",
    "# Check current memory status\n",
    "def check_memory_status():\n",
    "    \"\"\"Check current cache status\"\"\"\n",
    "    print(f\"Device: {device.upper()}\")\n",
    "    print(f\"Precision: {dtype}\")\n",
    "    print(f\"Cached models: {len(MODEL_CACHE)}\")\n",
    "    \n",
    "    if MODEL_CACHE:\n",
    "        print(\"\\nCached models:\")\n",
    "        for key in MODEL_CACHE:\n",
    "            print(f\"  - {key}\")\n",
    "    else:\n",
    "        print(\"\\nNo models currently cached\")\n",
    "\n",
    "# Check current status\n",
    "check_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this workshop, we've explored how to build a speech-to-text application using Whisper models with PyTorch on Intel XPU hardware. We've covered:\n",
    "\n",
    "1. **Setting up the environment** for Intel AI PCs\n",
    "2. **Loading and managing Whisper models** efficiently\n",
    "3. **Implementing streaming transcription** for real-time feedback\n",
    "4. **Building interactive applications** with audio generation\n",
    "5. **Supporting multiple languages** with transcription and translation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To continue your learning journey:\n",
    "\n",
    "1. Experiment with different model sizes and their trade-offs\n",
    "2. Integrate real-time microphone capture for live transcription\n",
    "3. Build REST APIs for transcription services\n",
    "4. Explore model quantization for edge deployment\n",
    "5. Combine with RAG systems for audio-based question answering\n",
    "\n",
    "The power of local AI processing on Intel AI PCs enables privacy-preserving, low-latency audio intelligence applications that can transform how we interact with spoken content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
