{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652ea6c8-8d13-4228-853e-fad46db470f5",
   "metadata": {},
   "source": [
    "# Running LLAMA3 on Intel AI PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0aeac-58b1-4114-95f1-7d3a7a4c34f2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to install Ollama on Windows with Intel GPUs. It applies to Intel Core Ultra and Core 11 - 14 gen integrated GPUs (iGPUs), as well as Intel Arc Series GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf7db8-9529-47dd-b41d-81b22c8d5848",
   "metadata": {},
   "source": [
    "## What is an AIPC\n",
    "\n",
    "What is an AI PC you ask?\n",
    "\n",
    "Here is an [explanation](https://www.intel.com/content/www/us/en/newsroom/news/what-is-an-ai-pc.htm#gs.a55so1):\n",
    "\n",
    "‚ÄùAn AI PC has a CPU, a GPU and an NPU, each with specific AI acceleration capabilities. An NPU, or neural processing unit, is a specialized accelerator that handles artificial intelligence (AI) and machine learning (ML) tasks right on your PC instead of sending data to be processed in the cloud. The GPU and CPU can also process these workloads, but the NPU is especially good at low-power AI calculations. The AI PC represents a fundamental shift in how our computers operate. It is not a solution for a problem that didn‚Äôt exist before. Instead, it promises to be a huge improvement for everyday PC usages.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682eb3e-540b-4814-8142-c54efc32f31b",
   "metadata": {},
   "source": [
    "## Install Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f8b6d2-34af-44ad-8363-dea57660bc00",
   "metadata": {},
   "source": [
    "### Step 1: System Preparation\n",
    "\n",
    "To set up your AIPC for running with Intel iGPUs, follow these essential steps:\n",
    "\n",
    "1. Update Intel GPU Drivers: Ensure your system has the latest Intel GPU drivers, which are crucial for optimal performance and compatibility. You can download these directly from Intel's [official website](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html) . Once you have installed the official drivers, you could also install Intel ARC Control to monitor the gpu:\n",
    "\n",
    "   <img src=\"Assets/gpu_arc_control.png\">\n",
    "\n",
    "\n",
    "2. Install Visual Studio 2022 Community edition with C++: Visual Studio 2022, along with the ‚ÄúDesktop Development with C++‚Äù workload, is required. This prepares your environment for C++ based extensions used by the intel SYCL backend that powers accelerated Ollama. You can download VS 2022 Community edition from the official site, [here](https://visualstudio.microsoft.com/downloads/).\n",
    "\n",
    "3. Install conda-forge: conda-forge will manage your Python environments and dependencies efficiently, providing a clean, minimal base for your Python setup. Visit conda-forge's [installation site](https://conda-forge.org/download/) to install for windows.\n",
    "\n",
    "4. Install Intel oneAPI Base Toolkit: The oneAPI Base Toolkit (specifically Intel‚Äô SYCL runtime, MKL and OneDNN) is essential for leveraging the performance enhancements offered by Intel's libraries and for ensuring that Ollama can fully utilize the GPU capabilities. By following these steps, your AI PC will be primed for running Ollama leveraging Intel iGPUs.\n",
    "\n",
    "```\n",
    "conda create -n llm-ollama python=3.11 -y\n",
    "conda activate llm-ollama\n",
    "conda install libuv -y\n",
    "pip install dpcpp-cpp-rt==2024.0.2 mkl-dpcpp==2024.0.0 onednn==2024.0.0\n",
    "\n",
    "```\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8040fd21-7782-4b97-a0eb-327816328f17",
   "metadata": {},
   "source": [
    "## Step 2: Install Ollama with Intel GPU support\n",
    "\n",
    "* Now that we have set up the environment, Intel GPU drivers, and runtime libraries, we can configure ollama to leverage the on-chip GPU.\n",
    "* Open miniforge prompt and run the below commands. We Install IPEX-LLM for llama.cpp and to use llama.cpp with IPEX-LLM, first ensure that ipex-llm[cpp] is installed.\n",
    "\n",
    "### With the ollama environment active, use pip to install required libraries for GPU. \n",
    "```\n",
    "conda activate llm-ollama\n",
    "pip install --pre --upgrade ipex-llm[cpp]\n",
    "```\n",
    "\n",
    "<img src=\"Assets/llm14.png\">\n",
    "\n",
    "* Create a folder ollama and navigate to the folder\n",
    "\n",
    "  ```\n",
    "  mkdir ollama\n",
    "  cd ollama\n",
    "  ```\n",
    "<img src=\"Assets/llm15.png\">\n",
    "\n",
    "* Open another miniforge prompt in administrator privilege mode and run the below command.\n",
    "    \n",
    "* Navigate to the above \"ollama\" folder that you created and run the below commands\n",
    "  \n",
    "    ```\n",
    "    conda activate llm-ollama\n",
    "    init-ollama.bat  # if init-ollama.bat is not available in your environment, restart your terminal\n",
    "\n",
    "    ```\n",
    "    <img src=\"Assets/llm17.png\">\n",
    "\n",
    "* Open another Miniforge prompt, navigate to the ollama folder where we created the symbolic links above and run the below command\n",
    "\n",
    "  ```\n",
    "  ollama serve\n",
    "\n",
    "  ```\n",
    "* ollama is now running in the backend and we should see as below\n",
    "\n",
    "  <img src=\"Assets/llm18.png\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca93a8-e995-4e0d-8361-0deb262fbe1c",
   "metadata": {},
   "source": [
    "## Run llama3 using Ollama on AI PC\n",
    "\n",
    "Now that we have installed Ollama, let‚Äôs see how to run llama 3 on your AI PC!\n",
    "Pull the Llama 3 8b from ollama repo:\n",
    "\n",
    "```\n",
    "ollama pull llama3\n",
    "\n",
    "```\n",
    "<img src=\"Assets/llm20.png\">\n",
    "\n",
    "*  Now, let‚Äôs create a custom llama 3 model and also configure all layers to be offloaded to the GPU.\n",
    "*  The main settings in the configuration file include num_gpu, which is set to 999 to ensure all layers utilize the GPU. We also configured the context length to 8192, the maximum supported by Llama 3.\n",
    "*  Additionally, we  customized the system prompt to add a more playful touch to the assistant (Pika :)). Here is a sample [Model file](Modelfile/Modelfile.llama3).\n",
    "\n",
    "<img src=\"Assets/model_file.png\">\n",
    "\n",
    "\n",
    "* Now that we have created a custom Modelfile, let‚Äôs create a custom model:\n",
    "\n",
    "```\n",
    "ollama create llama3-gpu -f Modelfile/Modelfile.llama3\n",
    "\n",
    "```\n",
    "\n",
    "* Let‚Äôs see if the model got created. The new model is ready to be run!.\n",
    "\n",
    "  <img src=\"Assets/llm21.png\">\n",
    "\n",
    "* Finally, now let‚Äôs run the model.\n",
    "```\n",
    "ollama run llama3-gpu\n",
    "\n",
    "```\n",
    "\n",
    "* As you can see above llama 3 is running on iGPU on the AI PC.\n",
    "\n",
    "<img src=\"Assets/llm22.png\">\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835dae5-ac6a-4a5a-bc12-49da5457dcb3",
   "metadata": {},
   "source": [
    "## Example code to run the models using streamlit on AI PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b94504-fcc8-454f-8a8d-b7312b7c0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/st_ollama.py\n",
    "import ollama\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Let's Chat....üêº\")\n",
    "\n",
    "# Load ollama models\n",
    "\n",
    "model_list = [model[\"name\"] for model in ollama.list()[\"models\"]]\n",
    "model = st.selectbox(\"Choose a model from the list\", model_list)\n",
    "\n",
    "if chat_input := st.chat_input(\"Hi, How are you?\"):\n",
    "    with st.spinner(\"Running....üêé\"):\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(chat_input)\n",
    "\n",
    "        def generate_response(user_input):\n",
    "            response = ollama.chat(model=model, messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': chat_input,\n",
    "            },\n",
    "            ],\n",
    "            stream=True,\n",
    "            )    \n",
    "            for res in response:\n",
    "                yield res[\"message\"][\"content\"]            \n",
    "        st.write_stream(generate_response(chat_input))\n",
    "        del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c1c8a-3355-4a1c-ae3e-06c4cb700ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run src/st_ollama.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae906f4-6fb4-451b-a7b7-d408db21c694",
   "metadata": {},
   "source": [
    "### Streamlit output runnling llama3\n",
    "\n",
    "Below is the screesnhot of llama3 is running on iGPU on the AI PC.\n",
    "\n",
    "<img src=\"Assets/ollama.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee0974-02d6-452d-8d84-2c3b683357df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "675b6c0f-a230-4413-91c4-c483b70530cb",
   "metadata": {},
   "source": [
    "* Reference: https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330c0d1-5369-41ac-a354-c2d8369c53a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
