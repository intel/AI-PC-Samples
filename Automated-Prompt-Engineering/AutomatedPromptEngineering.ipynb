{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6c301d-7aa6-424d-ba04-c4a69c90f458",
   "metadata": {},
   "source": [
    "# Automated Prompt Engineering with DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db260fec",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Purpose\n",
    "\n",
    "In order to make LLMs more performant for specific tasks, prompt engineering is used to instruct LLMs to complete certain tasks. Prompt engineering is cheaper than fine-tuning, takes less data, but has often been a manual task. For LLMs that are deployed on-device, which are smaller LLMs (usually fewer than 14 billion parameters), it is even more important to have goods prompts that are optimized for the task at hand as these LLMs cannot generalize like larger LLMs.\n",
    "\n",
    "We will use DSPy, an automatic prompt engineering framework to create a pipeline for a specific task and optimize the prompts for that task on IntelÂ® AI PC.\n",
    "\n",
    "### What is automated prompt engineering?\n",
    "\n",
    "Automatic prompt engineering is a technique that takes an LLM and automatically creates better and better prompts. Any automatic prompt engineering framework requires the following:\n",
    "- LLM that needs to be prompt-engineered\n",
    "- A dataset of inputs and outputs for the task at hand\n",
    "- A metric that measures how well the LLM is doing on the task\n",
    "\n",
    "Automatic prompt engineering frameworks then handle how to update the prompts to make the LLM perform better on the task. [DSPy](https://github.com/stanfordnlp/dspy) is one such framework that uses signatures, modules, and optimizers to create pipelines that can then be optimized by themselves. Another framework is [EvoPrompt](https://github.com/microsoft/EvoPrompt), which uses evolutionary algorithms to optimize prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0365a69",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing everything takes a couple of seconds. The below cell outlines the most pertinent imports for this sample:\n",
    "1. `llama_cpp` is a Python package that interacts with the llama.cpp library, which is a C++ implementation that runs LLMs and other models with a focus on speed and efficiency.\n",
    "2. `dspy` is a Python package that we will use for automated prompt engineering. We will explore how it works in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7c9c0-3f4a-4bbe-b1ef-21f0016dd556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:03:57.791540Z",
     "iopub.status.busy": "2024-11-26T02:03:57.790540Z",
     "iopub.status.idle": "2024-11-26T02:04:03.492329Z",
     "shell.execute_reply": "2024-11-26T02:04:03.492329Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a846f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:03.496339Z",
     "iopub.status.busy": "2024-11-26T02:04:03.496339Z",
     "iopub.status.idle": "2024-11-26T02:04:03.500233Z",
     "shell.execute_reply": "2024-11-26T02:04:03.500233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Misc Imports\n",
    "from datasets import load_dataset\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58ed8d-a459-4749-9f4d-c0bed2b71667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:03.505241Z",
     "iopub.status.busy": "2024-11-26T02:04:03.504242Z",
     "iopub.status.idle": "2024-11-26T02:04:03.508118Z",
     "shell.execute_reply": "2024-11-26T02:04:03.508118Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 1208\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de9e93",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset that we will be using is the [Riddle Sense dataset](https://inklab.usc.edu/RiddleSense/riddlesense_acl21_paper.pdf). This dataset contains riddles paired with multiple choice answers. The task is to predict the correct answer to the riddle. The dataset is available on Huggingface. The task for the LLM is to predict the correct multiple choice answer to solve the riddle.\n",
    "\n",
    "In many cases, one may not have a dataset ready for their task. For these cases, one would need to create examples themselves. DSPy can work with a few examples and then optimize the prompts for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35b605-d288-42c2-a1f4-f95424a02607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:03.511127Z",
     "iopub.status.busy": "2024-11-26T02:04:03.511127Z",
     "iopub.status.idle": "2024-11-26T02:04:04.234179Z",
     "shell.execute_reply": "2024-11-26T02:04:04.233651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load in he Riddle Sense dataset\n",
    "dataset = load_dataset(\"INK-USC/riddle_sense\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319f784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.237846Z",
     "iopub.status.busy": "2024-11-26T02:04:04.237846Z",
     "iopub.status.idle": "2024-11-26T02:04:04.375992Z",
     "shell.execute_reply": "2024-11-26T02:04:04.375992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the questions, answers, and choices from the dataset\n",
    "questions = [row[\"question\"] for row in dataset]\n",
    "answers = [row[\"answerKey\"] for row in dataset]\n",
    "choices = [row[\"choices\"] for row in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107f375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.379999Z",
     "iopub.status.busy": "2024-11-26T02:04:04.379999Z",
     "iopub.status.idle": "2024-11-26T02:04:04.384247Z",
     "shell.execute_reply": "2024-11-26T02:04:04.384247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pandas dataframe from the extracted data\n",
    "dataset = pd.DataFrame({\"question\": questions, \"answer\": answers, \"choices\": choices})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d313d9b",
   "metadata": {},
   "source": [
    "In order to make the sample run faster, we are downsampling the dataset by 90%. This is not recommended for real-world applications, but is done here to make the sample run faster. If one have a large amount of samples, DSPy also offers some level of finetuning. However, for many LLM applications, people often need to create some of their own responses, so we are showing how it works with smaller number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14d61e-606f-4c2b-b66a-c21c00888d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.388253Z",
     "iopub.status.busy": "2024-11-26T02:04:04.387254Z",
     "iopub.status.idle": "2024-11-26T02:04:04.391605Z",
     "shell.execute_reply": "2024-11-26T02:04:04.391605Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.sample(frac=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea929f2b-c702-4bf9-ae12-b89e286b9960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.394614Z",
     "iopub.status.busy": "2024-11-26T02:04:04.394614Z",
     "iopub.status.idle": "2024-11-26T02:04:04.408645Z",
     "shell.execute_reply": "2024-11-26T02:04:04.408645Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024f466",
   "metadata": {},
   "source": [
    "DSPy uses signatures to define the input and output for the LLM. This represented as a class in Python. Inside this class, we define the input and output for the LLM. The input is the riddle and the output is the correct answer to the riddle. We know that the correct answer for the LLM is the correct multiple choice answer to the riddle, and we can use Python typing to define this. DSPy will use this signature to prompt the LLM and also add prompts around this signature during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6824e9b-2262-451d-a1e6-26f799741953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.411653Z",
     "iopub.status.busy": "2024-11-26T02:04:04.411653Z",
     "iopub.status.idle": "2024-11-26T02:04:04.416950Z",
     "shell.execute_reply": "2024-11-26T02:04:04.415940Z"
    }
   },
   "outputs": [],
   "source": [
    "class Riddle(dspy.Signature):\n",
    "    \"\"\"Answer riddles by selecting the correct answer from a list of choices. Respond with the letter of the correct answer.\"\"\"  # noqa: E501\n",
    "\n",
    "    riddle = dspy.InputField()\n",
    "    answer: Literal[\"A\", \"B\", \"C\", \"D\", \"E\"] = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da4160",
   "metadata": {},
   "source": [
    "We need convert the list of riddles and answers to a format that DSPy can understand. DSPy takes in a list of `dspy.Example` objects that specify the riddle and the correct answer. We will convert the riddles and answers to this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c8255-a807-4117-bd28-3600233fa00a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.422946Z",
     "iopub.status.busy": "2024-11-26T02:04:04.422946Z",
     "iopub.status.idle": "2024-11-26T02:04:04.428859Z",
     "shell.execute_reply": "2024-11-26T02:04:04.428859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dspy_dataset = []\n",
    "\n",
    "for row in dataset.itertuples():\n",
    "    # Extract data from row\n",
    "    question = row.question\n",
    "    answer = row.answer\n",
    "    labels = row.choices[\"label\"]\n",
    "    context = row.choices[\"text\"]\n",
    "\n",
    "    # Create riddle input based on the question and answer choices\n",
    "    answer_choices = \"\"\n",
    "    for label, choice in zip(labels, context):\n",
    "        answer_choices += f\"{label}. {choice}, \"\n",
    "    answer_choices = answer_choices[:-2]  # Remove trailing comma\n",
    "    riddle = f\"{question}: {answer_choices}\"\n",
    "\n",
    "    # Create example\n",
    "    example = dspy.Example(riddle=riddle, answer=answer).with_inputs(\"riddle\")\n",
    "\n",
    "    # Append example to dataset\n",
    "    dspy_dataset.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf3075-22c5-4a15-aeac-d04c38742605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.431865Z",
     "iopub.status.busy": "2024-11-26T02:04:04.431865Z",
     "iopub.status.idle": "2024-11-26T02:04:04.435155Z",
     "shell.execute_reply": "2024-11-26T02:04:04.435155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "random.shuffle(dspy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d7ff6",
   "metadata": {},
   "source": [
    "In order to test the prompt found via DSPy, we need to create a train, validation, and test set. The train set is what DSPy will use to find prompts that work and the validation dataset will evaluate the prompts that DSPy finds. The test set will be used to evaluate the final prompt that DSPy finds as a hold-out set. This is analogous to the train, validation, and test set in machine learning.\n",
    "\n",
    "We will use a 60-20-20 split for the train, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e512966-3ce4-4912-9348-316693380ffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.439163Z",
     "iopub.status.busy": "2024-11-26T02:04:04.439163Z",
     "iopub.status.idle": "2024-11-26T02:04:04.442752Z",
     "shell.execute_reply": "2024-11-26T02:04:04.442752Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(0.6 * len(dspy_dataset))  # 60% for training\n",
    "val_size = int(0.2 * len(dspy_dataset))  # 20% for validation\n",
    "\n",
    "# Split the list\n",
    "train = dspy_dataset[:train_size]\n",
    "val = dspy_dataset[train_size : train_size + val_size]\n",
    "test = dspy_dataset[train_size + val_size :]  # Remaining 20% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db069d0",
   "metadata": {},
   "source": [
    "One of the main inputs for any prompt engineering framework is the LLM used. Here, we provide a couple of widely used SLMs that are available via huggingface and are performant on IntelÂ® AI PCs. Please use the dropdown to select the LLM that you would like to use for this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794e114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.445759Z",
     "iopub.status.busy": "2024-11-26T02:04:04.445759Z",
     "iopub.status.idle": "2024-11-26T02:04:04.449404Z",
     "shell.execute_reply": "2024-11-26T02:04:04.449404Z"
    }
   },
   "outputs": [],
   "source": [
    "model_to_repo = {\n",
    "    \"Phi-3.1-mini-4k-instruct-Q4_K_M.gguf\": \"bartowski/Phi-3.1-mini-4k-instruct-GGUF\",\n",
    "    \"Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\": \"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",  # noqa: E501\n",
    "    \"Llama-3.2-1B-Instruct-Q4_K_M.gguf\": \"bartowski/Llama-3.2-1B-Instruct-GGUF\",\n",
    "    \"qwen2-1_5b-instruct-q4_k_m.gguf\": \"Qwen/Qwen2-1.5B-Instruct-GGUF\",\n",
    "    \"qwen2-7b-instruct-q4_k_m.gguf\": \"Qwen/Qwen2-7B-Instruct-GGUF\",\n",
    "    \"qwen2-0_5b-instruct-q4_k_m.gguf\": \"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9ec7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.453410Z",
     "iopub.status.busy": "2024-11-26T02:04:04.452410Z",
     "iopub.status.idle": "2024-11-26T02:04:04.460014Z",
     "shell.execute_reply": "2024-11-26T02:04:04.460014Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_to_repo.keys(),\n",
    "    # Default to Qwen2 1.5B for best results\n",
    "    value=\"qwen2-1_5b-instruct-q4_k_m.gguf\",\n",
    "    description=\"Select an LLM:\",\n",
    ")\n",
    "\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2881216",
   "metadata": {},
   "source": [
    "After we select the LLM to be used, we will then load the LLM using `llama-cpp-python`. The `from_pretrained` function will download the model and tokenizer from Huggingface and load it onto the machine. We will then use this LLM to prompt the riddles and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52de8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:04.463020Z",
     "iopub.status.busy": "2024-11-26T02:04:04.463020Z",
     "iopub.status.idle": "2024-11-26T02:04:06.194342Z",
     "shell.execute_reply": "2024-11-26T02:04:06.194342Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=model_to_repo[model_dropdown.value],\n",
    "    filename=model_dropdown.value,\n",
    "    # This tells Llama.cpp to put 5 layers of the model on the GPU.\n",
    "    # The rest of the model will run on the CPU.\n",
    "    n_gpu_layers=5,\n",
    "    seed=SEED,\n",
    "    # Increase context window size to 4096 so that the model can see the entire riddle\n",
    "    # Having a large enough window size is important for the prompt optimization part\n",
    "    n_ctx=4096,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c60d2c",
   "metadata": {},
   "source": [
    "Once we have loaded the LLM, we need to configure DSPy to use this LLM. DSPy offers the `LlamaCPP` method which takes the `llm` object. DSPy will then use `llama-cpp-python` and the LLM to prompt the riddles and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca65204-0219-4ca6-989b-103bb03f0142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:06.199348Z",
     "iopub.status.busy": "2024-11-26T02:04:06.199348Z",
     "iopub.status.idle": "2024-11-26T02:04:06.203083Z",
     "shell.execute_reply": "2024-11-26T02:04:06.203083Z"
    }
   },
   "outputs": [],
   "source": [
    "llamalm = dspy.LlamaCpp(model=\"llama\", llama_model=llm, model_type=\"chat\", seed=SEED)\n",
    "dspy.settings.configure(lm=llamalm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e8229",
   "metadata": {},
   "source": [
    "The metric we will use for evaluating the LLM's performance is `answer_exact_match`, which returns `True` if the LLM answer matches the correct answer exactly and `False` otherwise. We will use this metric to evaluate the LLM's performance on the validation and test set.\n",
    "\n",
    "For more complex tasks (like RAG or summarization), `answer_exact_match` may not be a good metric. In those cases, one would need to use a metric that is more suited to the task at hand. `DSPy` offers [auto-evaluation metrics](https://github.com/stanfordnlp/dspy/blob/main/dspy/evaluate/auto_evaluation.py#L21) that prompts the LLM for a numeric evaluation. Other options include [`BLEU`](https://en.wikipedia.org/wiki/BLEU), [`ROUGE`](https://en.wikipedia.org/wiki/ROUGE_(metric)), or [`METEOR`](https://en.wikipedia.org/wiki/METEOR). [`BERTScore`](https://arxiv.org/abs/1904.09675) uses the [BERT language model](https://en.wikipedia.org/wiki/BERT_(language_model)) to evaluate the LLM's performance using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f98664-1f96-4f1d-beb9-0ba6e8b1b4bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:06.207094Z",
     "iopub.status.busy": "2024-11-26T02:04:06.207094Z",
     "iopub.status.idle": "2024-11-26T02:04:06.209862Z",
     "shell.execute_reply": "2024-11-26T02:04:06.209862Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = dspy.evaluate.metrics.answer_exact_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9151af",
   "metadata": {},
   "source": [
    "After we have our dataset, we then need to create a Module that represents our input and what prompt strategy the LLM should use. We will use the `Module` class from `dspy` to create a module that represents the input and output for the LLM. We will then use this module to create a pipeline that will be optimized by DSPy.\n",
    "\n",
    "Here, we make sure to use our `Riddle` signature to specify the input and output we want from the LLM. Then, we will use `dspy.ChainOfThought` to tell DSPy to use Chain-Of-Thought prompt-engineering strategy. Chain-Of-Thought is a prompt-engineering strategy that helps the LLM think step-by-step to solve reasoning tasks. Without DSPy, one would need to manually create Chain-Of-Thought prompts for the LLM to solve reasoning tasks. `dspy.ChainOfThought` will automatically create these prompts for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724fde89-a082-43ad-9d66-9f57928b021b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:06.213872Z",
     "iopub.status.busy": "2024-11-26T02:04:06.212873Z",
     "iopub.status.idle": "2024-11-26T02:04:06.217127Z",
     "shell.execute_reply": "2024-11-26T02:04:06.217127Z"
    }
   },
   "outputs": [],
   "source": [
    "class RiddleAnsweringAI(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.signature = Riddle\n",
    "        self.respond = dspy.ChainOfThought(self.signature)\n",
    "\n",
    "    def forward(self, riddle):\n",
    "        return self.respond(riddle=riddle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf4f77",
   "metadata": {},
   "source": [
    "Now that we have defined the inputs, outputs, and the LLM pipeline, we then need to have a strategy to evaluate the LLM's performance with new prompts. We use `dspy.Evaluate` to accept a dataset, metric, and start the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d84772-1218-4526-b75e-cf04d5514812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:06.221137Z",
     "iopub.status.busy": "2024-11-26T02:04:06.221137Z",
     "iopub.status.idle": "2024-11-26T02:04:06.225360Z",
     "shell.execute_reply": "2024-11-26T02:04:06.225360Z"
    }
   },
   "outputs": [],
   "source": [
    "train_evaluate = dspy.Evaluate(\n",
    "    devset=train, metric=metric, num_threads=1, display_progress=True, display_table=10\n",
    ")\n",
    "val_evaluate = dspy.Evaluate(\n",
    "    devset=val, metric=metric, num_threads=1, display_progress=True, display_table=10\n",
    ")\n",
    "test_evaluate = dspy.Evaluate(\n",
    "    devset=test, metric=metric, num_threads=1, display_progress=True, display_table=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb79e1",
   "metadata": {},
   "source": [
    "Before we start the optimization process, let's get a baseline of the LLM's performance on the train, validation, and test sets.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The following code cells will takes around 10-15 minutes to complete. Please be patient!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b56c5-6d2d-477b-add8-fe3074f98cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:04:06.229370Z",
     "iopub.status.busy": "2024-11-26T02:04:06.228371Z",
     "iopub.status.idle": "2024-11-26T02:10:38.342299Z",
     "shell.execute_reply": "2024-11-26T02:10:38.342299Z"
    }
   },
   "outputs": [],
   "source": [
    "orig_train_score = train_evaluate(RiddleAnsweringAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ca9f4-a8d8-412f-8475-f7519131437f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:10:38.346306Z",
     "iopub.status.busy": "2024-11-26T02:10:38.346306Z",
     "iopub.status.idle": "2024-11-26T02:13:12.291416Z",
     "shell.execute_reply": "2024-11-26T02:13:12.291416Z"
    }
   },
   "outputs": [],
   "source": [
    "orig_val_score = val_evaluate(RiddleAnsweringAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94df2a-260e-4151-b90d-f516ac45fd56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:13:12.296424Z",
     "iopub.status.busy": "2024-11-26T02:13:12.295424Z",
     "iopub.status.idle": "2024-11-26T02:16:15.364405Z",
     "shell.execute_reply": "2024-11-26T02:16:15.363395Z"
    }
   },
   "outputs": [],
   "source": [
    "orig_test_score = test_evaluate(RiddleAnsweringAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed764f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:16:15.367402Z",
     "iopub.status.busy": "2024-11-26T02:16:15.367402Z",
     "iopub.status.idle": "2024-11-26T02:16:15.377095Z",
     "shell.execute_reply": "2024-11-26T02:16:15.377095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the original scores\n",
    "print(f\"Original Training Score: {orig_train_score}\")\n",
    "print(f\"Original Validation Score: {orig_val_score}\")\n",
    "print(f\"Original Test Score: {orig_test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb9437",
   "metadata": {},
   "source": [
    "We can use the `dspy.inspect_history` to see what the input and response was by the LLM. This will help us understand what the LLM is doing and how it is solving the task. Let's take a look at the last two inputs and responses from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024602d8-18d3-4795-8997-e19e7d4edb32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:16:15.381103Z",
     "iopub.status.busy": "2024-11-26T02:16:15.381103Z",
     "iopub.status.idle": "2024-11-26T02:16:15.390135Z",
     "shell.execute_reply": "2024-11-26T02:16:15.390135Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = dspy.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373b158",
   "metadata": {},
   "source": [
    "DSPy offers a variety of different optimizers to find the best prompts. We will use the `MIPROv2` to find better prompts for the LLM. `MIPROv2` is a prompt-engineering optimizer. \n",
    "\n",
    "We use `MIPROv2` to do the following:\n",
    "1. Create some examples for the LLM to prompt\n",
    "2. Use few-shot prompting to help the LLM understand how to solve the task in hand\n",
    "3. Use the LLM to describe the dataset and create different dataset and task instructions\n",
    "4. Use bayesian optimization to find the best instructions for the LLM\n",
    "\n",
    "`MIPROv2` contains hyperparameters that control how long it takes to find prompts as well. We use the `light` setting for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d7657-ecef-43c9-b86f-46d3892228c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:16:15.395141Z",
     "iopub.status.busy": "2024-11-26T02:16:15.394141Z",
     "iopub.status.idle": "2024-11-26T02:16:15.398430Z",
     "shell.execute_reply": "2024-11-26T02:16:15.398430Z"
    }
   },
   "outputs": [],
   "source": [
    "optm = dspy.MIPROv2(metric=metric, auto=\"light\", num_threads=1, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f0c22",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "The following code cell will takes around 30-40 minutes to complete. Please be patient!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe55c30-3dc6-4489-835b-3ad80f7598af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:16:15.401438Z",
     "iopub.status.busy": "2024-11-26T02:16:15.401438Z",
     "iopub.status.idle": "2024-11-26T02:49:53.891769Z",
     "shell.execute_reply": "2024-11-26T02:49:53.891769Z"
    }
   },
   "outputs": [],
   "source": [
    "optimized_riddle_answerer = optm.compile(\n",
    "    RiddleAnsweringAI(),\n",
    "    trainset=train,\n",
    "    valset=val,\n",
    "    # The number of examples that is generated and included in the prompt\n",
    "    max_bootstrapped_demos=2,\n",
    "    # The number of examples from the training set that is included in the prompt\n",
    "    max_labeled_demos=2,\n",
    "    requires_permission_to_run=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761faa94",
   "metadata": {},
   "source": [
    "Now that we have the optimized riddle answerer, we can re-evaluate the pipeline on the train, validation, and test datasets. The metric for the test dataset will be the most important as the test dataset is a hold-out set that the LLM has never seen before.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The following code cells will takes around 10-15 minutes to complete. Please be patient!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e302391-55cb-4ed8-99e6-b2d1b9a3aa46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:49:53.894792Z",
     "iopub.status.busy": "2024-11-26T02:49:53.894792Z",
     "iopub.status.idle": "2024-11-26T02:57:29.679461Z",
     "shell.execute_reply": "2024-11-26T02:57:29.679461Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_train_score = train_evaluate(optimized_riddle_answerer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9d64b-1ad3-48d1-92a1-665d8b71a9d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:57:29.680971Z",
     "iopub.status.busy": "2024-11-26T02:57:29.680971Z",
     "iopub.status.idle": "2024-11-26T02:59:38.654073Z",
     "shell.execute_reply": "2024-11-26T02:59:38.654073Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_val_score = val_evaluate(optimized_riddle_answerer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27182da0-f96c-460f-bb9c-08fd3b9a1459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T02:59:38.655083Z",
     "iopub.status.busy": "2024-11-26T02:59:38.655083Z",
     "iopub.status.idle": "2024-11-26T03:01:55.805280Z",
     "shell.execute_reply": "2024-11-26T03:01:55.805280Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_test_score = test_evaluate(optimized_riddle_answerer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84506a3d",
   "metadata": {},
   "source": [
    "Let's take a look at the last two inputs and responses from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dspy.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6daff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:01:55.805280Z",
     "iopub.status.busy": "2024-11-26T03:01:55.805280Z",
     "iopub.status.idle": "2024-11-26T03:01:55.825697Z",
     "shell.execute_reply": "2024-11-26T03:01:55.825697Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original Training Score: {orig_train_score}\")\n",
    "print(f\"Optimized Training Score: {opt_train_score}\")\n",
    "print(f\"Original Validation Score: {orig_val_score}\")\n",
    "print(f\"Optimized Validation Score: {opt_val_score}\")\n",
    "print(f\"Original Test Score: {orig_test_score}\")\n",
    "print(f\"Optimized Test Score: {opt_test_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "5339b0a04e6e4c658a2c75f017d35461": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a27bd9e2a164532a06782509dcb6c67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "99a7eb081d7d4896976e91c1a9a9249d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "Phi-3.1-mini-4k-instruct-Q4_K_M.gguf",
        "Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
        "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
        "qwen2-1_5b-instruct-q4_k_m.gguf",
        "qwen2-7b-instruct-q4_k_m.gguf",
        "qwen2-0_5b-instruct-q4_k_m.gguf"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Select an LLM:",
       "description_allow_html": false,
       "disabled": false,
       "index": 3,
       "layout": "IPY_MODEL_5339b0a04e6e4c658a2c75f017d35461",
       "style": "IPY_MODEL_8a27bd9e2a164532a06782509dcb6c67",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
